{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of SMPA with Gradient descent and PyTorch here only y moves and with Pchip it is pretty good and beating industry standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BSpline(nn.Module):\n",
    "    \"\"\"Differentiable B-spline implementation in PyTorch\"\"\"\n",
    "    def __init__(self, knots, degree=3):\n",
    "        super(BSpline, self).__init__()\n",
    "        self.knots = knots  # Knot vector\n",
    "        self.degree = degree\n",
    "\n",
    "    def basis(self, t, i, k):\n",
    "        \"\"\"Recursive De Boor basis function\"\"\"\n",
    "        if k == 0:\n",
    "            return torch.where((self.knots[i] <= t) & (t < self.knots[i+1]),\n",
    "                              torch.ones_like(t), torch.zeros_like(t))\n",
    "\n",
    "        d1 = (self.knots[i+k] - self.knots[i])\n",
    "        d2 = (self.knots[i+k+1] - self.knots[i+1])\n",
    "\n",
    "        # Avoid division by zero\n",
    "        c1 = torch.zeros_like(t)\n",
    "        mask1 = d1 > 0\n",
    "        c1[mask1] = (t[mask1] - self.knots[i]) / d1 * self.basis(t[mask1], i, k-1)\n",
    "\n",
    "        c2 = torch.zeros_like(t)\n",
    "        mask2 = d2 > 0\n",
    "        c2[mask2] = (self.knots[i+k+1] - t[mask2]) / d2 * self.basis(t[mask2], i+1, k-1)\n",
    "\n",
    "        return c1 + c2\n",
    "\n",
    "    def forward(self, t, control_points):\n",
    "        n = len(control_points) - 1\n",
    "        result = torch.zeros_like(t)\n",
    "\n",
    "        for i in range(n + 1):\n",
    "            result += control_points[i] * self.basis(t, i, self.degree)\n",
    "\n",
    "        return result\n",
    "\n",
    "class TorchCubicSpline(nn.Module):\n",
    "    \"\"\"Differentiable cubic spline implementation using PyTorch\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        super(TorchCubicSpline, self).__init__()\n",
    "        self.x = x  # Control point x-coordinates (fixed)\n",
    "        self.y = nn.Parameter(y)  # Control point y-coordinates (learnable)\n",
    "\n",
    "        # Compute knot vector for cubic B-spline\n",
    "        n = len(x)\n",
    "        self.knots = torch.cat([\n",
    "            x[0].repeat(3),\n",
    "            x[1:-1],\n",
    "            x[-1].repeat(3)\n",
    "        ])\n",
    "\n",
    "        self.bspline = BSpline(self.knots, degree=3)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Ensure t is clamped to domain\n",
    "        t_clamped = torch.clamp(t, self.x[0], self.x[-1])\n",
    "\n",
    "        # Evaluate B-spline\n",
    "        return self.bspline(t_clamped, self.y)\n",
    "\n",
    "class DifferentiablePchip(nn.Module):\n",
    "    \"\"\"Fully differentiable PCHIP (monotonic) spline in PyTorch\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        super(DifferentiablePchip, self).__init__()\n",
    "        self.x = x  # Control point x-coordinates (fixed)\n",
    "        self.y = nn.Parameter(y)  # Control point y-coordinates (learnable)\n",
    "        self.n = len(x) - 1\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Find the segment for each input t\n",
    "        idx = torch.searchsorted(self.x, t) - 1\n",
    "        idx = torch.clamp(idx, 0, self.n - 1)\n",
    "\n",
    "        # Get x and y values for the segment\n",
    "        x0 = self.x[idx]\n",
    "        x1 = self.x[idx + 1]\n",
    "        y0 = torch.gather(self.y, 0, idx)\n",
    "        y1 = torch.gather(self.y, 0, idx + 1)\n",
    "\n",
    "        # Normalize t within segment\n",
    "        t_norm = (t - x0) / (x1 - x0)\n",
    "\n",
    "        # Calculate slopes\n",
    "        dy = self.y[1:] - self.y[:-1]\n",
    "        dx = self.x[1:] - self.x[:-1]\n",
    "        slopes = dy / dx\n",
    "\n",
    "        # Enforce monotonicity by adjusting derivatives\n",
    "        # Simplified PCHIP algorithm\n",
    "        d = torch.zeros_like(self.y)\n",
    "        for i in range(1, len(self.y)-1):\n",
    "            if slopes[i-1] * slopes[i] > 0:\n",
    "                # Same sign, use harmonic mean for monotonicity\n",
    "                w1 = 2*dx[i] + dx[i-1]\n",
    "                w2 = dx[i] + 2*dx[i-1]\n",
    "                d[i] = (w1 + w2) / (w1/slopes[i-1] + w2/slopes[i])\n",
    "            else:\n",
    "                # Different signs or one is zero\n",
    "                d[i] = 0\n",
    "\n",
    "        # Handle endpoints\n",
    "        d[0] = slopes[0]\n",
    "        d[-1] = slopes[-1]\n",
    "\n",
    "        # Get derivatives for segment\n",
    "        d0 = torch.gather(d, 0, idx)\n",
    "        d1 = torch.gather(d, 0, idx + 1)\n",
    "\n",
    "        # Hermite basis functions\n",
    "        h00 = 2*t_norm**3 - 3*t_norm**2 + 1\n",
    "        h10 = t_norm**3 - 2*t_norm**2 + t_norm\n",
    "        h01 = -2*t_norm**3 + 3*t_norm**2\n",
    "        h11 = t_norm**3 - t_norm**2\n",
    "\n",
    "        # Apply cubic Hermite interpolation with monotonic derivatives\n",
    "        dx_segment = x1 - x0\n",
    "        result = (h00 * y0 + h10 * dx_segment * d0 +\n",
    "                 h01 * y1 + h11 * dx_segment * d1)\n",
    "\n",
    "        return result\n",
    "\n",
    "class PyTorchGradientSMPA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.05, epochs=100, random_state=7, verbose=False,\n",
    "                 lambda_reg=0.0001, patience=10, decay_factor=0.9, min_learning_rate=1e-6,\n",
    "                 n_control_points=6, smoothing_factor=0.0001, spline_type='cubic',\n",
    "                 device=None, track_history=False, optimizer_type='adam', scheduler_type='reduce_on_plateau'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.track_history = track_history\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.scheduler_type = scheduler_type\n",
    "\n",
    "        if spline_type not in ['cubic', 'pchip']:\n",
    "            raise ValueError(\"spline_type must be 'cubic' or 'pchip'\")\n",
    "\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def _to_tensor(self, data, dtype=torch.float32):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.to(self.device).to(dtype)\n",
    "        elif isinstance(data, pd.Series):\n",
    "            # Convert pandas Series to numpy array first\n",
    "            return torch.tensor(data.values, dtype=dtype, device=self.device)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            # Convert pandas DataFrame to numpy array first\n",
    "            return torch.tensor(data.values, dtype=dtype, device=self.device)\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            # Handle numpy arrays\n",
    "            return torch.tensor(data, dtype=dtype, device=self.device)\n",
    "        else:\n",
    "            # For any other array-like objects\n",
    "            try:\n",
    "                return torch.tensor(np.array(data), dtype=dtype, device=self.device)\n",
    "            except:\n",
    "                raise ValueError(f\"Cannot convert {type(data)} to tensor: {data}\")\n",
    "\n",
    "    def _calculate_class_means(self, X, y):\n",
    "        mask_1 = y == 1\n",
    "        self.m1 = torch.mean(X[mask_1], dim=0)\n",
    "        self.m0 = torch.mean(X[~mask_1], dim=0)\n",
    "\n",
    "    def _initialize_control_points(self, X):\n",
    "        n_features = X.shape[1] - 1  # All features except the last one\n",
    "        self.spline_models = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_features):\n",
    "            x_min, x_max = X[:, i].min().item(), X[:, i].max().item()\n",
    "            control_x = torch.linspace(x_min, x_max, self.n_control_points, device=self.device)\n",
    "\n",
    "            # Initialize y values around the mean of the target feature\n",
    "            y_min, y_max = X[:, -1].min().item(), X[:, -1].max().item()\n",
    "            y_mid = (self.m0[-1] + self.m1[-1]) / 2\n",
    "            y_range = y_max - y_min\n",
    "            control_y = torch.empty(self.n_control_points, device=self.device).uniform_(\n",
    "                y_mid - y_range * 0.05, y_mid + y_range * 0.05\n",
    "            )\n",
    "\n",
    "            if self.spline_type == 'cubic':\n",
    "                spline = TorchCubicSpline(control_x, control_y).to(self.device)\n",
    "            else:\n",
    "                spline = DifferentiablePchip(control_x, control_y).to(self.device)\n",
    "\n",
    "            self.spline_models.append(spline)\n",
    "\n",
    "        # Store initial control points for plotting or history\n",
    "        self.initial_control_points = [(m.x.clone(), m.y.clone()) for m in self.spline_models]\n",
    "\n",
    "    def _calculate_displacement(self, X):\n",
    "        \"\"\"Calculate total displacement as sum of spline contributions minus target feature\"\"\"\n",
    "        total_spline = torch.zeros(X.shape[0], device=self.device)\n",
    "        for i, spline in enumerate(self.spline_models):\n",
    "            total_spline += spline(X[:, i])\n",
    "        return X[:, -1] - total_spline\n",
    "\n",
    "    def _update_pseudo_labels(self, X, y):\n",
    "        m1_reshaped = self.m1.unsqueeze(0)\n",
    "        m0_reshaped = self.m0.unsqueeze(0)\n",
    "        m1_displacement = self._calculate_displacement(m1_reshaped)[0]\n",
    "\n",
    "        self.class_1_pseudo = 1 if m1_displacement > 0 else -1\n",
    "        self.class_0_pseudo = -self.class_1_pseudo\n",
    "\n",
    "        return torch.where(y == 1,\n",
    "                         torch.tensor(self.class_1_pseudo, device=self.device),\n",
    "                         torch.tensor(self.class_0_pseudo, device=self.device))\n",
    "\n",
    "    def _create_optimizer_and_scheduler(self):\n",
    "        params = [p for spline in self.spline_models for p in spline.parameters()]\n",
    "        if self.optimizer_type.lower() == 'adam':\n",
    "            optimizer = torch.optim.Adam(params, lr=self.initial_learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(params, lr=self.initial_learning_rate)\n",
    "\n",
    "        if self.scheduler_type.lower() == 'reduce_on_plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=self.decay_factor,\n",
    "                patience=self.patience, min_lr=self.min_learning_rate, verbose=False\n",
    "            )\n",
    "        elif self.scheduler_type.lower() == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=self.patience, gamma=self.decay_factor\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if not set(np.unique(y)).issubset({0, 1}):\n",
    "            raise ValueError(\"Labels must be 0 and 1\")\n",
    "        if X.shape[1] < 2:\n",
    "            raise ValueError(\"At least 2 features required (one for spline, one as target)\")\n",
    "\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        y_tensor = self._to_tensor(y, dtype=torch.long)\n",
    "\n",
    "        self._calculate_class_means(X_tensor, y_tensor)\n",
    "        self._initialize_control_points(X_tensor)\n",
    "\n",
    "        optimizer, scheduler = self._create_optimizer_and_scheduler()\n",
    "\n",
    "        best_error = float('inf')\n",
    "        best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "        best_class_1_pseudo = None\n",
    "\n",
    "        if self.track_history:\n",
    "            self.error_history_ = []\n",
    "            self.control_point_history = [self.initial_control_points]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            pseudo_labels = self._update_pseudo_labels(X_tensor, y_tensor)\n",
    "            displacements = self._calculate_displacement(X_tensor)\n",
    "\n",
    "            errors = displacements * pseudo_labels <= 0\n",
    "            error_count = errors.sum().item()\n",
    "\n",
    "            if self.verbose and epoch % 5 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch}: Errors = {error_count}, LR = {current_lr:.6f}\")\n",
    "\n",
    "            if error_count < best_error:\n",
    "                best_error = error_count\n",
    "                best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "                best_class_1_pseudo = self.class_1_pseudo\n",
    "                self.best_epoch = epoch\n",
    "\n",
    "                if error_count == 0 and epoch > 10:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Perfect separation achieved at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            if self.track_history:\n",
    "                self.error_history_.append(error_count)\n",
    "                self.control_point_history.append(\n",
    "                    [(s.x.clone().cpu().numpy(), s.y.clone().detach().cpu().numpy())\n",
    "                     for s in self.spline_models]\n",
    "                )\n",
    "\n",
    "            if error_count == 0:\n",
    "                continue\n",
    "\n",
    "            error_indices = torch.where(errors)[0]\n",
    "            X_err = X_tensor[error_indices]\n",
    "            y_err = y_tensor[error_indices]\n",
    "\n",
    "            ti = torch.where(y_err == 1,\n",
    "                           torch.tensor(1, device=self.device),\n",
    "                           torch.tensor(-1, device=self.device))\n",
    "\n",
    "            spline_values = torch.zeros(X_err.shape[0], device=self.device)\n",
    "            for i, spline in enumerate(self.spline_models):\n",
    "                spline_values += spline(X_err[:, i])\n",
    "\n",
    "            loss = torch.mean(torch.relu(1.0 - ti * self.class_1_pseudo * (X_err[:, -1] - spline_values)))\n",
    "\n",
    "            if self.lambda_reg > 0:\n",
    "                smoothness_penalty = 0\n",
    "                for spline in self.spline_models:\n",
    "                    y_diff = spline.y[1:] - spline.y[:-1]\n",
    "                    x_diff = spline.x[1:] - spline.x[:-1]\n",
    "                    smoothness_penalty += torch.mean((y_diff / x_diff)**2)\n",
    "                loss += self.lambda_reg * smoothness_penalty\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(error_count)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if optimizer.param_groups[0]['lr'] <= self.min_learning_rate:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Minimum learning rate reached at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        for spline, best_y in zip(self.spline_models, best_control_ys):\n",
    "            spline.y.data = best_y\n",
    "        self.class_1_pseudo = best_class_1_pseudo\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        predictions = torch.where(displacements > 0,\n",
    "                                torch.tensor(1 if self.class_1_pseudo > 0 else 0, device=self.device),\n",
    "                                torch.tensor(0 if self.class_1_pseudo > 0 else 1, device=self.device))\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        raw_probs = 1 / (1 + torch.exp(-displacements * self.class_1_pseudo * 0.5))\n",
    "\n",
    "        if self.class_1_pseudo > 0:\n",
    "            probs = torch.column_stack([1 - raw_probs, raw_probs])\n",
    "        else:\n",
    "            probs = torch.column_stack([raw_probs, 1 - raw_probs])\n",
    "\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def plot_convergence(self, figsize=(10, 4)):\n",
    "        if not self.track_history or not hasattr(self, 'error_history_'):\n",
    "            print(\"Convergence plotting requires track_history=True and a fitted model.\")\n",
    "            return None\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.plot(self.error_history_, 'b-', label='Errors')\n",
    "        ax.set_title('Error Convergence')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Number of Errors')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

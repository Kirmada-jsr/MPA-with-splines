{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.interpolate import CubicSpline, PchipInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 100  # Increase animation size limit\n",
    "\n",
    "class SMPAEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Single Smooth Manifold Projection Algorithm estimator that will be used in the ensemble.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.005, epochs=50, random_state=None, verbose=False,\n",
    "                 lambda_scaling='log', patience=5, decay_factor=0.5, min_learning_rate=1e-6,\n",
    "                 n_control_points=5, smoothing_factor=0.1, spline_type='cubic'):\n",
    "        \"\"\"\n",
    "        Initialize the SMPA estimator.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float, default=0.005\n",
    "            Initial learning rate for gradient updates\n",
    "        epochs : int, default=50\n",
    "            Maximum number of training epochs\n",
    "        random_state : int, default=None\n",
    "            Random seed for reproducibility\n",
    "        verbose : bool, default=False\n",
    "            Whether to print progress during training\n",
    "        lambda_scaling : {'log', 'sqrt', 'none'}, default='log'\n",
    "            Method to scale the update based on distance\n",
    "        patience : int, default=5\n",
    "            Epochs to wait before reducing learning rate\n",
    "        decay_factor : float, default=0.5\n",
    "            Factor to reduce learning rate by when patience is exceeded\n",
    "        min_learning_rate : float, default=1e-6\n",
    "            Minimum learning rate to stop training\n",
    "        n_control_points : int, default=5\n",
    "            Number of control points for the spline\n",
    "        smoothing_factor : float, default=0.1\n",
    "            Smoothing parameter for the spline\n",
    "        spline_type : {'cubic', 'pchip'}, default='cubic'\n",
    "            Type of spline to use ('pchip' preserves monotonicity better)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_scaling = lambda_scaling\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "\n",
    "        # Validation\n",
    "        if lambda_scaling not in ['log', 'sqrt', 'none']:\n",
    "            raise ValueError(\"lambda_scaling must be one of 'log', 'sqrt', or 'none'\")\n",
    "        if spline_type not in ['cubic', 'pchip']:\n",
    "            raise ValueError(\"spline_type must be one of 'cubic' or 'pchip'\")\n",
    "\n",
    "        # History tracking\n",
    "        #self.error_history_ = []\n",
    "        #self.learning_rate_history_ = []\n",
    "        #self.control_point_history = []\n",
    "        #self.displacement_history = []\n",
    "        #self.error_index_history = []\n",
    "\n",
    "        # Set random seed\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _calculate_class_means(self, X, y):\n",
    "        \"\"\"Calculate the mean point for each class.\"\"\"\n",
    "        mask_1 = y == 1\n",
    "        self.m1 = X[mask_1].mean(axis=0)\n",
    "        self.m0 = X[~mask_1].mean(axis=0)\n",
    "\n",
    "    def _initialize_control_points(self, X):\n",
    "        \"\"\"Initialize control points with improved boundary handling.\"\"\"\n",
    "        # Data range with padding to prevent boundary issues\n",
    "        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "        y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "        x_range = x_max - x_min\n",
    "        y_range = y_max - y_min\n",
    "\n",
    "        # Add padding outside data range for better boundary behavior\n",
    "        x_min_extended = x_min - 0.2 * x_range\n",
    "        x_max_extended = x_max + 0.2 * x_range\n",
    "\n",
    "        # Create evenly spaced control points\n",
    "        self.control_x = np.linspace(x_min_extended, x_max_extended, self.n_control_points)\n",
    "\n",
    "        # Initialize control y values near the midpoint between class means\n",
    "        y_mid = (self.m0[1] + self.m1[1]) / 2\n",
    "\n",
    "        # Add slight randomization but keep endpoints stable\n",
    "        self.control_y = np.random.uniform(\n",
    "            y_mid - y_range * 0.05,\n",
    "            y_mid + y_range * 0.05,\n",
    "            self.n_control_points\n",
    "        )\n",
    "\n",
    "        # Make sure boundary control points have stable derivatives (flatter)\n",
    "        # This helps prevent the spline from shooting up at the edges\n",
    "        if self.n_control_points > 2:\n",
    "            # Set the first and last control points to match second and second-to-last\n",
    "            self.control_y[0] = self.control_y[1]\n",
    "            self.control_y[-1] = self.control_y[-2]\n",
    "\n",
    "        # Store initial control points for visualization\n",
    "        self.initial_control_x = self.control_x.copy()\n",
    "        self.initial_control_y = self.control_y.copy()\n",
    "        #self.control_point_history.append((self.control_x.copy(), self.control_y.copy()))\n",
    "\n",
    "    def _fit_spline(self):\n",
    "        \"\"\"Fit a spline to the current control points with boundary constraints.\"\"\"\n",
    "        if self.spline_type == 'cubic':\n",
    "            # Use cubic spline with clamped boundary conditions (zero derivative at endpoints)\n",
    "            self.spline = CubicSpline(\n",
    "                self.control_x,\n",
    "                self.control_y,\n",
    "                bc_type='clamped'  # This forces zero first derivative at endpoints\n",
    "            )\n",
    "        else:  # 'pchip'\n",
    "            # PCHIP preserves monotonicity and reduces oscillations\n",
    "            self.spline = PchipInterpolator(self.control_x, self.control_y)\n",
    "\n",
    "    def _calculate_displacement(self, X):\n",
    "        \"\"\"Calculate vertical displacement from the spline.\"\"\"\n",
    "        spline_y = self.spline(X[:, 0])\n",
    "        return X[:, 1] - spline_y\n",
    "\n",
    "    def _update_pseudo_labels(self, X, y):\n",
    "        \"\"\"Determine which side of the spline each class should be on.\"\"\"\n",
    "        m1_displacement = self._calculate_displacement(self.m1.reshape(1, -1))[0]\n",
    "        self.class_1_pseudo = 1 if m1_displacement > 0 else -1\n",
    "        self.class_0_pseudo = -self.class_1_pseudo\n",
    "        return np.where(y == 1, self.class_1_pseudo, self.class_0_pseudo)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the estimator to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (0 or 1)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self.classes_ = np.unique(y)\n",
    "        if not set(self.classes_).issubset({0, 1}):\n",
    "            raise ValueError(\"Labels must be 0 and 1\")\n",
    "        if X.shape[1] != 2:\n",
    "            raise ValueError(\"This is a 2D-only algorithm for now!\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Reset training history\n",
    "        #self.control_point_history = []\n",
    "        #self.displacement_history = []\n",
    "        #self.error_index_history = []\n",
    "        #self.error_history_ = []\n",
    "        #self.learning_rate_history_ = []\n",
    "\n",
    "        # Initialize\n",
    "        self._calculate_class_means(X, y)\n",
    "        self._initialize_control_points(X)\n",
    "        self._fit_spline()\n",
    "\n",
    "        # Track best model\n",
    "        best_error = float('inf')\n",
    "        best_control_x = None\n",
    "        best_control_y = None\n",
    "        best_class_1_pseudo = None\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        patience_counter = 0\n",
    "        current_learning_rate = self.initial_learning_rate\n",
    "\n",
    "        # Pre-compute class indices for efficient updates\n",
    "        indices_class_0 = np.where(y == 0)[0]\n",
    "        indices_class_1 = np.where(y == 1)[0]\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            # Update spline and calculate errors\n",
    "            self._fit_spline()\n",
    "            pseudo_labels = self._update_pseudo_labels(X, y)\n",
    "            displacements = self._calculate_displacement(X)\n",
    "            errors = (displacements * pseudo_labels <= 0)\n",
    "            error_count = np.sum(errors)\n",
    "\n",
    "            # Store history\n",
    "            #self.error_history_.append(error_count)\n",
    "            #self.learning_rate_history_.append(current_learning_rate)\n",
    "            #self.displacement_history.append(displacements.copy())\n",
    "            #self.error_index_history.append(np.where(errors)[0].copy())\n",
    "\n",
    "            # Verbose logging\n",
    "            if self.verbose and epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: Errors = {error_count}, LR = {current_learning_rate:.6f}\")\n",
    "\n",
    "            # Update best model if improved\n",
    "            if error_count < best_error:\n",
    "                best_error = error_count\n",
    "                best_control_x = self.control_x.copy()\n",
    "                best_control_y = self.control_y.copy()\n",
    "                best_class_1_pseudo = self.class_1_pseudo\n",
    "                patience_counter = 0\n",
    "                self.best_epoch = epoch\n",
    "            else:\n",
    "                # Learning rate decay\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    current_learning_rate = max(current_learning_rate * self.decay_factor, self.min_learning_rate)\n",
    "                    patience_counter = 0\n",
    "                    if current_learning_rate == self.min_learning_rate:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Min learning rate reached at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            # Update control points based on errors\n",
    "            if errors.any():\n",
    "                error_indices = np.where(errors)[0]\n",
    "\n",
    "                for idx in error_indices:\n",
    "                    # Get the misclassified point\n",
    "                    d = X[idx]\n",
    "\n",
    "                    # Find nearest control point\n",
    "                    distances = np.abs(self.control_x - d[0])\n",
    "                    nearest_idx = np.argmin(distances)\n",
    "                    distance = distances[nearest_idx]\n",
    "\n",
    "                    # Scale update based on distance\n",
    "                    if self.lambda_scaling == 'log':\n",
    "                        lmbda = np.log1p(distance)\n",
    "                    elif self.lambda_scaling == 'sqrt':\n",
    "                        lmbda = np.sqrt(distance)\n",
    "                    else:  # 'none'\n",
    "                        lmbda = distance\n",
    "\n",
    "                    # Adaptive margin: higher margins for points far from the decision boundary\n",
    "                    margin = max(0.1, min(1.0, lmbda * 0.2))\n",
    "\n",
    "                    # Determine opposite class's correctly classified points\n",
    "                    if y[idx] == 1:  # Misclassified class 1 point\n",
    "                        opp_indices = indices_class_0\n",
    "                    else:  # Misclassified class 0 point\n",
    "                        opp_indices = indices_class_1\n",
    "\n",
    "                    # Find correctly classified points in opposite class\n",
    "                    opp_displacements = displacements[opp_indices]\n",
    "                    opp_labels = pseudo_labels[opp_indices]\n",
    "                    correct_opp = opp_indices[opp_displacements * opp_labels > margin]  # Use margin\n",
    "\n",
    "                    # Calculate step direction\n",
    "                    if len(correct_opp) > 0:\n",
    "                        # Pick a random subset of correctly classified opposite class points\n",
    "                        n_random = max(1, min(len(correct_opp) // 5, 5))  # Take 20%, max 5 points\n",
    "                        random_correct = np.random.choice(correct_opp, size=n_random, replace=False)\n",
    "\n",
    "                        # Average position of the random subset\n",
    "                        random_avg_opp = np.mean(X[random_correct], axis=0)\n",
    "\n",
    "                        # Calculate step direction toward this average\n",
    "                        delta_x = random_avg_opp[0] - self.control_x[nearest_idx]\n",
    "                        delta_y = random_avg_opp[1] - self.control_y[nearest_idx]\n",
    "\n",
    "                        # Apply step with learning rate\n",
    "                        step_x = delta_x * current_learning_rate / (1 + lmbda)\n",
    "                        step_y = delta_y * current_learning_rate / (1 + lmbda)\n",
    "                    else:\n",
    "                        # Fallback: Move control point vertically to correct the error\n",
    "                        step_x = 0\n",
    "                        step_y = -pseudo_labels[idx] * margin * current_learning_rate\n",
    "\n",
    "                    # Constrain step_x to maintain ascending order of control points\n",
    "                    if nearest_idx > 0:\n",
    "                        min_allowed_x = self.control_x[nearest_idx - 1] + 1e-6\n",
    "                        max_step_left = self.control_x[nearest_idx] - min_allowed_x\n",
    "                        step_x = max(step_x, -max_step_left)\n",
    "                    if nearest_idx < len(self.control_x) - 1:\n",
    "                        max_allowed_x = self.control_x[nearest_idx + 1] - 1e-6\n",
    "                        max_step_right = max_allowed_x - self.control_x[nearest_idx]\n",
    "                        step_x = min(step_x, max_step_right)\n",
    "\n",
    "                    # Apply updates\n",
    "                    self.control_x[nearest_idx] += step_x\n",
    "                    self.control_y[nearest_idx] += step_y\n",
    "\n",
    "                    # Special handling for boundary control points\n",
    "                    # Keep first and last control points moving less to prevent boundary issues\n",
    "                    if nearest_idx == 0 or nearest_idx == len(self.control_x) - 1:\n",
    "                        # Scale down the step for boundary points\n",
    "                        self.control_y[nearest_idx] *= 0.7  # Dampen movement at boundaries\n",
    "\n",
    "                        # If it's the first control point, make second point match the derivative\n",
    "                        if nearest_idx == 0 and len(self.control_x) > 2:\n",
    "                            delta = self.control_y[1] - self.control_y[0]\n",
    "                            self.control_y[0] = self.control_y[1] - 0.5 * delta\n",
    "\n",
    "                        # If it's the last control point, make second-to-last point match the derivative\n",
    "                        elif nearest_idx == len(self.control_x) - 1 and len(self.control_x) > 2:\n",
    "                            delta = self.control_y[-1] - self.control_y[-2]\n",
    "                            self.control_y[-1] = self.control_y[-2] + 0.5 * delta\n",
    "\n",
    "            # Store control points after all updates\n",
    "            #self.control_point_history.append((self.control_x.copy(), self.control_y.copy()))\n",
    "\n",
    "        # Store the last control points from the final epoch\n",
    "        self.last_control_x = self.control_x.copy()\n",
    "        self.last_control_y = self.control_y.copy()\n",
    "\n",
    "        # Restore best model for prediction\n",
    "        self.control_x = best_control_x\n",
    "        self.control_y = best_control_y\n",
    "        self._fit_spline()\n",
    "        self.class_1_pseudo = best_class_1_pseudo\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The predicted classes (0 or 1)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        displacements = self._calculate_displacement(X)\n",
    "        return np.where(displacements > 0,\n",
    "                        1 if self.class_1_pseudo > 0 else 0,\n",
    "                        0 if self.class_1_pseudo > 0 else 1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate probability of each class for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        proba : array-like, shape (n_samples, 2)\n",
    "            The class probabilities\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        displacements = self._calculate_displacement(X)\n",
    "\n",
    "        # Convert displacements to probabilities with sigmoid\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Scale displacement to probability (use absolute displacement as confidence)\n",
    "        raw_probs = sigmoid(displacements * self.class_1_pseudo * 0.5)\n",
    "\n",
    "        # Create probability array [p(class=0), p(class=1)]\n",
    "        if self.class_1_pseudo > 0:\n",
    "            probs = np.column_stack([1 - raw_probs, raw_probs])\n",
    "        else:\n",
    "            probs = np.column_stack([raw_probs, 1 - raw_probs])\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "class EnsembleSMPA(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Ensemble of Smooth Manifold Projection Algorithms with majority voting.\n",
    "    This classifier uses multiple smooth spline boundaries to separate binary classes in 2D.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=5, bootstrap=True, bootstrap_features=False, \n",
    "                 max_features=1.0, max_samples=1.0, random_state=None, verbose=False,\n",
    "                 learning_rate=0.005, epochs=50, lambda_scaling='log', patience=5, \n",
    "                 decay_factor=0.5, min_learning_rate=1e-6, n_control_points=5, \n",
    "                 smoothing_factor=0.1, spline_type='cubic'):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleSMPA classifier.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int, default=5\n",
    "            Number of SMPA estimators in the ensemble\n",
    "        bootstrap : bool, default=True\n",
    "            Whether to use bootstrap sampling for estimators\n",
    "        bootstrap_features : bool, default=False\n",
    "            Whether to use bootstrap sampling for features\n",
    "        max_features : float or int, default=1.0\n",
    "            Fraction of features to use for each estimator if bootstrap_features=True\n",
    "        max_samples : float or int, default=1.0\n",
    "            Fraction of samples to use for each estimator if bootstrap=True\n",
    "        random_state : int, default=None\n",
    "            Random seed for reproducibility\n",
    "        verbose : bool, default=False\n",
    "            Whether to print progress during training\n",
    "        learning_rate : float, default=0.005\n",
    "            Initial learning rate for gradient updates\n",
    "        epochs : int, default=50\n",
    "            Maximum number of training epochs\n",
    "        lambda_scaling : {'log', 'sqrt', 'none'}, default='log'\n",
    "            Method to scale the update based on distance\n",
    "        patience : int, default=5\n",
    "            Epochs to wait before reducing learning rate\n",
    "        decay_factor : float, default=0.5\n",
    "            Factor to reduce learning rate by when patience is exceeded\n",
    "        min_learning_rate : float, default=1e-6\n",
    "            Minimum learning rate to stop training\n",
    "        n_control_points : int, default=5\n",
    "            Number of control points for the spline\n",
    "        smoothing_factor : float, default=0.1\n",
    "            Smoothing parameter for the spline\n",
    "        spline_type : {'cubic', 'pchip'}, default='cubic'\n",
    "            Type of spline to use ('pchip' preserves monotonicity better)\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.max_features = max_features\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_scaling = lambda_scaling\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "\n",
    "        # Set random seed\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the ensemble classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (0 or 1)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self.classes_ = np.unique(y)\n",
    "        if not set(self.classes_).issubset({0, 1}):\n",
    "            raise ValueError(\"Labels must be 0 and 1\")\n",
    "        if X.shape[1] != 2:\n",
    "            raise ValueError(\"This is a 2D-only algorithm for now!\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Determine sample and feature sizes for bootstrapping\n",
    "        if isinstance(self.max_samples, float):\n",
    "            max_samples = int(self.max_samples * n_samples)\n",
    "        else:\n",
    "            max_samples = min(self.max_samples, n_samples)\n",
    "            \n",
    "        if isinstance(self.max_features, float):\n",
    "            max_features = max(1, int(self.max_features * n_features))\n",
    "        else:\n",
    "            max_features = min(self.max_features, n_features)\n",
    "        \n",
    "        # Create ensemble of estimators\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            if self.verbose:\n",
    "                print(f\"Training estimator {i+1}/{self.n_estimators}\")\n",
    "            \n",
    "            # Create a random seed for this estimator\n",
    "            estimator_seed = self.rng.randint(0, 10000) if self.random_state is not None else None\n",
    "            \n",
    "            # Create sample indices for this estimator\n",
    "            if self.bootstrap:\n",
    "                sample_indices = self.rng.choice(n_samples, max_samples, replace=True)\n",
    "            else:\n",
    "                sample_indices = np.arange(n_samples)\n",
    "                \n",
    "            # Create feature indices for this estimator\n",
    "            if self.bootstrap_features:\n",
    "                feature_indices = self.rng.choice(n_features, max_features, replace=False)\n",
    "                # Ensure we have at least 2 features for 2D algorithm\n",
    "                if len(feature_indices) < 2:\n",
    "                    feature_indices = np.arange(min(2, n_features))\n",
    "            else:\n",
    "                feature_indices = np.arange(n_features)\n",
    "            \n",
    "            # Extract bootstrap samples and features\n",
    "            X_bootstrap = X[sample_indices][:, feature_indices]\n",
    "            y_bootstrap = y[sample_indices]\n",
    "            \n",
    "            # Initialize and fit the estimator\n",
    "            estimator = SMPAEstimator(\n",
    "                learning_rate=self.learning_rate,\n",
    "                epochs=self.epochs,\n",
    "                random_state=estimator_seed,\n",
    "                verbose=self.verbose,\n",
    "                lambda_scaling=self.lambda_scaling,\n",
    "                patience=self.patience,\n",
    "                decay_factor=self.decay_factor,\n",
    "                min_learning_rate=self.min_learning_rate,\n",
    "                n_control_points=self.n_control_points,\n",
    "                smoothing_factor=self.smoothing_factor,\n",
    "                spline_type=self.spline_type\n",
    "            )\n",
    "            \n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "            \n",
    "            # Store the estimator along with its sample and feature indices\n",
    "            self.estimators_.append({\n",
    "                'estimator': estimator,\n",
    "                'sample_indices': sample_indices,\n",
    "                'feature_indices': feature_indices\n",
    "            })\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X using majority voting.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The predicted classes (0 or 1)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        predictions = np.zeros((X.shape[0], len(self.estimators_)))\n",
    "        \n",
    "        # Collect predictions from all estimators\n",
    "        for i, estimator_dict in enumerate(self.estimators_):\n",
    "            estimator = estimator_dict['estimator']\n",
    "            # Use the full feature set for prediction\n",
    "            predictions[:, i] = estimator.predict(X)\n",
    "        \n",
    "        # Majority voting\n",
    "        return np.round(np.mean(predictions, axis=1)).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate probability of each class for samples in X by averaging probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        proba : array-like, shape (n_samples, 2)\n",
    "            The class probabilities\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        probas = np.zeros((n_samples, 2))\n",
    "        \n",
    "        # Collect probabilities from all estimators\n",
    "        for estimator_dict in self.estimators_:\n",
    "            estimator = estimator_dict['estimator']\n",
    "            # Add this estimator's probabilities\n",
    "            probas += estimator.predict_proba(X)\n",
    "        \n",
    "        # Average probabilities across estimators\n",
    "        probas /= len(self.estimators_)\n",
    "        \n",
    "        return probas\n",
    "        \n",
    "    def visualize(self, X, y, figsize=(10, 10)):\n",
    "        \"\"\"\n",
    "        Visualize the ensemble decision boundaries.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values\n",
    "        figsize : tuple, default=(10, 10)\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib figure\n",
    "            The visualization figure\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Create a figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot data points\n",
    "        ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', label='Class 0', alpha=0.7)\n",
    "        ax.scatter(X[y == 1, 0], X[y == 1, 1], c='coral', label='Class 1', alpha=0.7)\n",
    "        \n",
    "        # Create a mesh grid to visualize decision boundaries\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                             np.arange(y_min, y_max, 0.02))\n",
    "        \n",
    "        # Plot decision boundaries for each estimator\n",
    "        for i, estimator_dict in enumerate(self.estimators_):\n",
    "            estimator = estimator_dict['estimator']\n",
    "            \n",
    "            # Plot the spline curve\n",
    "            x_curve = np.linspace(x_min, x_max, 100)\n",
    "            y_curve = estimator.spline(x_curve)\n",
    "            ax.plot(x_curve, y_curve, 'k-', alpha=0.3, linewidth=1)\n",
    "            \n",
    "        # Plot the ensemble decision boundary\n",
    "        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = self.predict(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot the decision boundary with contour\n",
    "        contour = ax.contour(xx, yy, Z, levels=[0.5], colors='k', linewidths=2)\n",
    "        \n",
    "        # Plot the probability heatmap\n",
    "        proba = self.predict_proba(mesh_points)[:, 1]\n",
    "        proba = proba.reshape(xx.shape)\n",
    "        contourf = ax.contourf(xx, yy, proba, levels=np.linspace(0, 1, 11), \n",
    "                               cmap='RdBu_r', alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(contourf, ax=ax)\n",
    "        cbar.set_label('Probability of Class 1')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_title('Ensemble SMPA Decision Boundary')\n",
    "        ax.legend()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moon_dataset(n_points=400, noise=0.2):\n",
    "    X, y = make_moons(n_samples=n_points, random_state=7, noise=noise)\n",
    "    return X, y\n",
    "\n",
    "class ClassifierComparison:\n",
    "    def __init__(self, X, y, random_state=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.random_state = random_state\n",
    "        # We won't create a single split upfront anymore\n",
    "        # but will keep track of class distribution for reference\n",
    "        self.grids = {\n",
    "            'smpa': {\n",
    "                'learning_rate': [0.001, 0.005],\n",
    "                'epochs': [300],\n",
    "                'n_control_points': [7, 8, 9, 10],\n",
    "                'decay_factor': [0.99, 0.999],\n",
    "                'lambda_scaling': ['log', 'sqrt'],\n",
    "                'patience' : [5, 10],\n",
    "                'n_estimators' : [5, 10, 50]\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': [0.1, 1, 10, 50, 100],\n",
    "                'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "                'kernel': ['rbf']\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': [5, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        }\n",
    "        # Define scaler types but don't fit them yet\n",
    "        self.scaler_types = {\n",
    "            'smpa': MinMaxScaler(feature_range=(-100, 100)),\n",
    "            'svm': StandardScaler(),\n",
    "            'rf': StandardScaler(),\n",
    "            'dt': StandardScaler()\n",
    "        }\n",
    "\n",
    "    def create_train_test_split(self, seed):\n",
    "        \"\"\"Create a fresh train-test split with the given random seed\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=seed, stratify=self.y\n",
    "        )\n",
    "        print(f\"Train Class Dist - {np.bincount(y_train)}\")\n",
    "        print(f\"Test Class Dist - {np.bincount(y_test)}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def scale_data(self, X_train, X_test, classifier_type):\n",
    "        \"\"\"Scale data using a fresh scaler for the given classifier type\"\"\"\n",
    "        # Create a new scaler instance of the appropriate type\n",
    "        scaler = self.scaler_types[classifier_type].__class__(**self.scaler_types[classifier_type].get_params())\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "\n",
    "    def grid_search(self, X_train_scaled, y_train, classifier_type='smpa', seed=9):\n",
    "        \"\"\"Perform grid search with the provided scaled training data\"\"\"\n",
    "        classifiers = {\n",
    "            'smpa': EnsembleSMPA(random_state=seed),\n",
    "            'svm': SVC(random_state=seed),\n",
    "            'rf': RandomForestClassifier(random_state=seed),\n",
    "            'dt': DecisionTreeClassifier(random_state=seed),\n",
    "        }\n",
    "        clf = classifiers[classifier_type]\n",
    "        param_grid = self.grids[classifier_type]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "        grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=10, verbose=3)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        return grid_search\n",
    "\n",
    "        # Modify the stability_test method in ClassifierComparison class\n",
    "    def stability_test(self, classifier_type='smpa', n_runs=5, save_plots=True, show_plots=False):\n",
    "        scores = []\n",
    "        best_params_list = []\n",
    "        \n",
    "        for seed in range(n_runs):\n",
    "            print(f\"\\nRun {seed} for {classifier_type}...\")\n",
    "            \n",
    "            # Create a fresh train-test split for each run\n",
    "            X_train, X_test, y_train, y_test = self.create_train_test_split(seed)\n",
    "            \n",
    "            # Scale the data for this specific split\n",
    "            X_train_scaled, X_test_scaled = self.scale_data(X_train, X_test, classifier_type)\n",
    "            \n",
    "            # Grid search on this fresh train split with newly scaled data\n",
    "            grid_search = self.grid_search(X_train_scaled, y_train, classifier_type, seed)\n",
    "            clf = grid_search.best_estimator_\n",
    "            best_params_list.append(grid_search.best_params_)\n",
    "            print(f\"Run {seed} Best Params: {grid_search.best_params_}\")\n",
    "            \n",
    "            try:\n",
    "                y_pred = clf.predict(X_test_scaled)\n",
    "                score = accuracy_score(y_test, y_pred)\n",
    "                scores.append(score)\n",
    "                \n",
    "                # if classifier_type == 'smpa':\n",
    "                #     # Plot with the same scaled data used for training\n",
    "                #     try:\n",
    "                #         fig = clf.visualize(X_train_scaled, y_train)\n",
    "                #         if save_plots:\n",
    "                #             # Save the figure instead of displaying it\n",
    "                #             plt.savefig(f\"{classifier_type}_run_{seed}.png\", dpi=100)\n",
    "                #         if show_plots:\n",
    "                #             plt.show()\n",
    "                #         # Always close the figure to free memory\n",
    "                #         plt.close(fig)\n",
    "                #     except Exception as e:\n",
    "                #         print(f\"ðŸš¨ Error in visualization for run {seed}: {e}\")\n",
    "                \n",
    "                print(f\"Run {seed} Score: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸš¨ Error in run {seed}: {e}\")\n",
    "                scores.append(0)\n",
    "        \n",
    "        return {\n",
    "            'mean_score': np.mean(scores),\n",
    "            'std_score': np.std(scores),\n",
    "            'scores': scores,\n",
    "            'best_params_list': best_params_list\n",
    "        }\n",
    "    def statistical_significance_test(self, baseline_scores_dict, target='smpa'):\n",
    "        target_scores = baseline_scores_dict[target]['scores']\n",
    "        for clf_type, results in baseline_scores_dict.items():\n",
    "            if clf_type != target:\n",
    "                t_stat, p_val = stats.ttest_ind(target_scores, results['scores'])\n",
    "                print(f\"\\nðŸ”¬ {target} vs. {clf_type}:\")\n",
    "                print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "                print(f\"P-Value: {p_val:.4f}\")\n",
    "                print(\"ðŸ† Significant difference!\" if p_val < 0.05 else \"ðŸ¤ No significant difference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0 for smpa...\n",
      "Train Class Dist - [160 160]\n",
      "Test Class Dist - [40 40]\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.858 total time=  10.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.907 total time=  10.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.868 total time=  15.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.860 total time=  20.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.860 total time=  17.1s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  17.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.916 total time=  18.7s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.858 total time=  23.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  32.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.879 total time=  34.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.879 total time=  33.7s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.858 total time=  31.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=  13.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=  15.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=   7.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=  22.2s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.840 total time=  22.6s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.849 total time=  18.6s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.860 total time=  27.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.860 total time=  38.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.849 total time=  38.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.849 total time= 1.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.869 total time=  40.8s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.860 total time=  34.1s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.849 total time=  30.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.868 total time= 2.2min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.879 total time= 2.4min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.850 total time=   8.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.888 total time= 2.6min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.869 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.869 total time=  18.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.879 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.849 total time=  15.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.840 total time=  12.1s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.850 total time=  20.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.879 total time=  19.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.850 total time=  43.3s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.849 total time=  43.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.869 total time=  45.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.860 total time=  42.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.849 total time=  30.9s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.869 total time=  42.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.879 total time= 2.7min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.869 total time= 3.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.849 total time= 2.9min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.860 total time=  13.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.860 total time= 3.3min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.849 total time= 2.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.897 total time=  16.8s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.840 total time=  23.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.860 total time=  21.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.897 total time=  20.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.869 total time= 3.7min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.840 total time=  22.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.888 total time=  39.6s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.850 total time=  44.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.869 total time=  37.6s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.840 total time=  46.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.888 total time=  43.8s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.849 total time= 2.5min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.840 total time=  50.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.869 total time= 3.4min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.860 total time= 3.7min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.849 total time= 3.3min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.860 total time= 3.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.869 total time= 3.7min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.897 total time=  54.9s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.907 total time=  44.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.877 total time=  41.8s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.897 total time=  43.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.897 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.877 total time=  43.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.887 total time= 1.2min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.916 total time= 1.3min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.869 total time= 4.2min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time= 1.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.879 total time= 4.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.849 total time= 4.0min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.869 total time= 4.2min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.879 total time= 4.2min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.879 total time= 2.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.849 total time= 4.1min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.916 total time= 1.3min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.897 total time=  24.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.916 total time=  34.9s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.877 total time= 1.5min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.877 total time=  23.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.897 total time=  24.9s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.877 total time=  23.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.888 total time=  38.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.897 total time=  58.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.916 total time=  55.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.887 total time=  49.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.897 total time= 1.6min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.887 total time=  59.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.907 total time= 1.3min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.896 total time= 6.4min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.916 total time= 6.5min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.887 total time= 3.7min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.869 total time= 7.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.888 total time= 7.2min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.916 total time= 7.1min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.916 total time=  33.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.896 total time=  23.6s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.896 total time= 7.2min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.897 total time=  34.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.916 total time=  26.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 4.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.916 total time=  28.3s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.887 total time=  32.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 6.2min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.858 total time=  55.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.916 total time= 1.0min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.907 total time= 1.2min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.868 total time=  46.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.916 total time= 1.1min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.916 total time= 5.9min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.916 total time= 1.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.907 total time=  25.9s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.907 total time=  27.9s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.877 total time= 3.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.849 total time=  11.5s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.916 total time=  16.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.907 total time=  24.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.858 total time=  19.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.916 total time= 4.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.858 total time=  56.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.916 total time= 1.0min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.897 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.868 total time=  40.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.897 total time= 1.4min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.916 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.877 total time= 4.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.916 total time= 5.1min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.916 total time= 5.2min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.868 total time= 5.1min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.916 total time= 6.0min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.850 total time=  24.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.869 total time=  21.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.907 total time= 5.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.840 total time=  17.8s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.849 total time=  10.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.850 total time=  19.8s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.888 total time=  19.5s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  25.7s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.849 total time=  34.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.850 total time=  30.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.841 total time=  43.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.869 total time=  30.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.897 total time= 4.5min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.916 total time= 3.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.858 total time=  24.2s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.858 total time= 4.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.858 total time= 2.6min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.897 total time= 4.1min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=   8.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=  12.7s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.840 total time=   7.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=  10.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.916 total time= 3.4min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.840 total time=  15.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=  22.3s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.860 total time=  29.2s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.849 total time=  27.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.860 total time=  27.8s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.860 total time=  41.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.849 total time= 1.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.860 total time=  33.8s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.849 total time=  29.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.849 total time= 2.1min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.860 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.860 total time=   7.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.869 total time= 2.8min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.869 total time= 2.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.879 total time=  18.3s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.840 total time=  17.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.860 total time=  11.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.879 total time=  12.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.869 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.840 total time=   5.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.860 total time=  22.6s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.860 total time=  18.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.869 total time=  31.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.869 total time=  22.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.840 total time=  42.1s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.840 total time=  27.8s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.849 total time= 2.0min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.860 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.860 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.869 total time= 3.0min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.850 total time=  13.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.888 total time=  16.1s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.849 total time= 2.9min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.860 total time= 3.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.840 total time=  20.3s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.860 total time=  15.2s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.840 total time=  12.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.888 total time=  25.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.850 total time=  40.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.888 total time=  38.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.850 total time=  29.2s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.840 total time=  36.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.860 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.888 total time=  36.3s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.840 total time=  35.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.860 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.840 total time= 2.8min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.860 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.869 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.840 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.879 total time=  24.5s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.907 total time=  21.6s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.877 total time=  18.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.877 total time=  18.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.888 total time=  30.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.907 total time=  26.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.916 total time=  33.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  52.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.877 total time=  41.3s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.869 total time= 1.2min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.877 total time=  49.8s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.869 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.840 total time= 2.5min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.916 total time= 1.0min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.869 total time= 3.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.840 total time= 2.5min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.907 total time=  12.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.869 total time= 2.9min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.879 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.877 total time=   9.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.916 total time=  25.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.897 total time=  16.0s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.877 total time=  15.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.907 total time=  19.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.916 total time=  21.7s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.916 total time=  37.3s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.887 total time=  33.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.888 total time=  43.7s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.887 total time=  26.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.907 total time=  32.9s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.916 total time= 3.5min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.887 total time= 3.4min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.916 total time= 3.6min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.869 total time= 4.4min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.887 total time= 3.7min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.896 total time=   8.1s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.916 total time=  17.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.877 total time= 2.3min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.888 total time=  20.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.897 total time=  16.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.925 total time=  22.9s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.896 total time=  14.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 3.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.877 total time=  33.4s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.879 total time= 4.8min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.916 total time=  47.2s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.916 total time=  38.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.907 total time=  50.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.916 total time=  36.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.877 total time=  35.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 4.2min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.916 total time=  22.9s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.877 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.907 total time= 4.3min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.858 total time=  10.5s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.916 total time=  15.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.925 total time=  12.2s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.907 total time=  20.6s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.916 total time= 3.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.858 total time=  13.5s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.858 total time=  29.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.916 total time=  38.6s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.907 total time=  48.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.897 total time=  44.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.858 total time=  21.4s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.916 total time=  42.3s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.916 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.868 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.868 total time= 2.7min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.916 total time= 3.4min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.916 total time= 3.1min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.860 total time=  18.2s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.916 total time=  17.3s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.860 total time=  13.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.868 total time=  20.5s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.916 total time= 3.6min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.868 total time=  13.0s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.916 total time=  21.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.879 total time=  32.8s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.877 total time=  28.4s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.879 total time=  28.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.888 total time=  35.7s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.877 total time=  33.9s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.888 total time=  41.1s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.916 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.858 total time= 2.4min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.858 total time= 2.9min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.916 total time= 3.3min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=  14.1s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.860 total time=  16.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5;, score=0.849 total time=  17.1s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=  11.8s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.860 total time=  17.0s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.916 total time= 3.5min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10;, score=0.858 total time=  10.9s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=sqrt, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=10;, score=0.907 total time= 3.6min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.879 total time=  30.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.849 total time=  30.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5;, score=0.860 total time=  35.8s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.879 total time=  35.8s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.888 total time= 2.7min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.879 total time= 2.7min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.868 total time= 2.0min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.860 total time=  30.9s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=5;, score=0.868 total time= 2.4min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=10;, score=0.849 total time=  38.0s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.879 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.850 total time=  13.7s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.879 total time=  15.6s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=5;, score=0.840 total time=  14.9s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.860 total time=  13.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.879 total time=  17.0s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=50, patience=10;, score=0.888 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=5, patience=10;, score=0.840 total time=  19.5s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.888 total time=  25.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.869 total time=  35.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=5;, score=0.849 total time=  28.6s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.879 total time=  38.1s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.888 total time=  36.5s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=10, patience=10;, score=0.849 total time=  36.6s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.888 total time= 2.5min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.849 total time= 2.3min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.888 total time= 2.5min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.849 total time= 2.5min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=10;, score=0.869 total time= 2.6min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.860 total time=  19.8s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.897 total time=  13.5s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=50, patience=5;, score=0.869 total time= 3.0min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=5;, score=0.840 total time=  17.1s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.860 total time=  14.3s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.888 total time=  16.4s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=5, patience=10;, score=0.840 total time=  16.5s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.869 total time=  33.7s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.840 total time=  34.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.860 total time=  36.8s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=5;, score=0.888 total time=  40.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.879 total time= 2.6min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.888 total time=  30.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.849 total time= 2.3min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=10, patience=10;, score=0.849 total time=  35.7s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=5;, score=0.869 total time= 2.8min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.879 total time= 2.5min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.849 total time= 2.2min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.877 total time=  21.9s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=9, n_estimators=50, patience=10;, score=0.869 total time= 2.6min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.907 total time=  30.3s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=5;, score=0.897 total time=  34.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.907 total time=  25.8s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.887 total time=  27.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=5, patience=10;, score=0.897 total time=  34.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.879 total time= 1.1min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.897 total time=  59.6s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=5;, score=0.877 total time=  57.3s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.879 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.849 total time= 2.6min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.879 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.879 total time= 2.5min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=5;, score=0.869 total time= 3.0min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.869 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.907 total time= 1.2min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=10, patience=10;, score=0.877 total time=  50.2s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=10, n_estimators=50, patience=10;, score=0.849 total time= 2.9min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.877 total time=  14.8s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.897 total time=  28.5s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=5;, score=0.916 total time=  21.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.916 total time=  18.0s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.897 total time=  11.9s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=5, patience=10;, score=0.887 total time=  14.1s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.916 total time=  30.2s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.887 total time=  37.1s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=5;, score=0.897 total time=  50.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.907 total time=  49.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.916 total time=  26.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=10, patience=10;, score=0.887 total time=  29.3s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.916 total time= 4.2min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.896 total time= 4.3min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.887 total time= 2.4min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.916 total time= 4.4min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.896 total time= 4.5min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.897 total time=  23.7s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.916 total time=  19.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=5;, score=0.896 total time=  17.0s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.916 total time=  18.9s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=10;, score=0.869 total time= 5.1min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=7, n_estimators=50, patience=5;, score=0.869 total time= 5.4min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.916 total time=  17.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=5, patience=10;, score=0.887 total time=  16.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 3.5min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.868 total time=  36.8s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=5;, score=0.916 total time= 4.0min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.916 total time=  47.0s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.916 total time=  44.6s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=5;, score=0.916 total time=  54.0s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.916 total time=  51.0s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=10, patience=10;, score=0.868 total time=  39.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.925 total time= 4.0min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.925 total time=  15.8s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.907 total time=  25.0s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=5;, score=0.849 total time=  14.9s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.907 total time=  15.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.849 total time=   8.8s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=5, patience=10;, score=0.925 total time=  14.1s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.877 total time= 2.8min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.868 total time=  25.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.916 total time=  42.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=5;, score=0.916 total time=  44.0s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=8, n_estimators=50, patience=10;, score=0.916 total time= 3.8min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.858 total time=  24.4s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.916 total time=  46.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=10, patience=10;, score=0.888 total time=  48.6s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.877 total time= 3.0min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.916 total time= 3.5min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.916 total time= 3.7min\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.877 total time= 3.8min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.860 total time=  10.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=5;, score=0.925 total time= 4.2min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.907 total time=  21.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5;, score=0.858 total time=  15.7s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.860 total time=  12.1s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.907 total time=  16.4s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10;, score=0.858 total time=  17.2s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=9, n_estimators=50, patience=10;, score=0.916 total time= 4.5min\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  28.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.858 total time=  21.6s\n",
      "[CV 1/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.869 total time=  28.3s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=log, learning_rate=0.005, n_control_points=10, n_estimators=50, patience=5;, score=0.858 total time= 2.6min\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5;, score=0.869 total time=  43.1s\n",
      "[CV 2/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.869 total time=  31.4s\n",
      "[CV 3/3] END decay_factor=0.999, epochs=300, lambda_scaling=sqrt, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10;, score=0.858 total time=  27.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m comparison\u001b[38;5;241m.\u001b[39mgrid_search \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMethodType(grid_search_modified, comparison)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_type \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Set save_plots=True and show_plots=False to avoid VS Code renderer issues\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     results[clf_type] \u001b[38;5;241m=\u001b[39m \u001b[43mcomparison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstability_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_type, res \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Stability:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m, in \u001b[0;36mClassifierComparison.stability_test\u001b[0;34m(self, classifier_type, n_runs, save_plots, show_plots)\u001b[0m\n\u001b[1;32m     89\u001b[0m X_train_scaled, X_test_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_data(X_train, X_test, classifier_type)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Grid search on this fresh train split with newly scaled data\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m clf \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     94\u001b[0m best_params_list\u001b[38;5;241m.\u001b[39mappend(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mgrid_search_modified\u001b[0;34m(self, X_train_scaled, y_train, classifier_type, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Limit the number of parallel jobs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(clf, param_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "X, y = generate_moon_dataset(n_points=400, noise=0.3)\n",
    "comparison = ClassifierComparison(X, y)\n",
    "\n",
    "# Run all classifiers on same split\n",
    "classifiers = ['smpa', 'svm', 'rf', 'dt']\n",
    "results = {}\n",
    "\n",
    "# Reduce the computation load for grid search\n",
    "# Modify the GridSearchCV in the grid_search method\n",
    "def grid_search_modified(self, X_train_scaled, y_train, classifier_type='smpa', seed=0):\n",
    "    \"\"\"Perform grid search with the provided scaled training data\"\"\"\n",
    "    classifiers = {\n",
    "        'smpa': EnsembleSMPA(random_state=seed),\n",
    "        'svm': SVC(random_state=seed),\n",
    "        'rf': RandomForestClassifier(random_state=seed),\n",
    "        'dt': DecisionTreeClassifier(random_state=seed),\n",
    "    }\n",
    "    clf = classifiers[classifier_type]\n",
    "    param_grid = self.grids[classifier_type]\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    # Limit the number of parallel jobs\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=10, verbose=3)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    return grid_search\n",
    "\n",
    "# Replace the original method with the modified one\n",
    "comparison.grid_search = types.MethodType(grid_search_modified, comparison)\n",
    "\n",
    "for clf_type in classifiers:\n",
    "    # Set save_plots=True and show_plots=False to avoid VS Code renderer issues\n",
    "    results[clf_type] = comparison.stability_test(clf_type, n_runs=5, save_plots=True, show_plots=False)\n",
    "\n",
    "for clf_type, res in results.items():\n",
    "    print(f\"\\nðŸ“Š {clf_type.upper()} Stability:\")\n",
    "    print(f\"Mean Score: {res['mean_score']:.4f}\")\n",
    "    print(f\"Score Std Dev: {res['std_score']:.4f}\")\n",
    "\n",
    "comparison.statistical_significance_test(results, target='smpa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d176d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class OptimizedDifferentiablePchip(nn.Module):\n",
    "    def __init__(self, x_min, x_max, y_init, n_control_points):\n",
    "        super().__init__()\n",
    "        # Store min and max for the feature range\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.n = n_control_points\n",
    "        # Learnable parameters for x-coordinate increments\n",
    "        self.theta = nn.Parameter(torch.zeros(n_control_points - 1))\n",
    "        # Y-coordinates as learnable parameters\n",
    "        self.y = nn.Parameter(y_init.clone().detach())\n",
    "        self.d = None  # To store precomputed derivatives\n",
    "        self.x = None  # To store computed x control points\n",
    "\n",
    "    def update_spline(self):\n",
    "        \"\"\"Compute x control points and derivatives based on current parameters.\"\"\"\n",
    "        # Compute x coordinates parametrically\n",
    "        delta = F.softplus(self.theta)  # Ensure positive increments\n",
    "        cum_sum = torch.cumsum(torch.cat([torch.tensor([0.0], device=self.theta.device), delta]), dim=0)\n",
    "        total_sum = cum_sum[-1]\n",
    "        self.x = self.x_min + (self.x_max - self.x_min) * cum_sum / total_sum\n",
    "        # Compute derivatives\n",
    "        dy = self.y[1:] - self.y[:-1]\n",
    "        dx = self.x[1:] - self.x[:-1]\n",
    "        slopes = dy / dx\n",
    "        d = torch.zeros_like(self.y)\n",
    "        for i in range(1, len(self.y) - 1):\n",
    "            if slopes[i - 1] * slopes[i] > 0:\n",
    "                w1 = 2 * dx[i] + dx[i - 1]\n",
    "                w2 = dx[i] + 2 * dx[i - 1]\n",
    "                d[i] = (w1 + w2) / (w1 / slopes[i - 1] + w2 / slopes[i])\n",
    "        d[0] = slopes[0]\n",
    "        d[-1] = slopes[-1]\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, t):\n",
    "        if self.x is None or self.d is None:\n",
    "            self.update_spline()  # Ensure spline is updated\n",
    "        t = t.contiguous()\n",
    "        idx = torch.clamp(torch.searchsorted(self.x, t) - 1, 0, self.n - 2)\n",
    "        x0 = self.x[idx]\n",
    "        x1 = self.x[idx + 1]\n",
    "        y0 = self.y[idx]\n",
    "        y1 = self.y[idx + 1]\n",
    "        t_norm = (t - x0) / (x1 - x0)\n",
    "        d0 = self.d[idx]\n",
    "        d1 = self.d[idx + 1]\n",
    "        t2 = t_norm * t_norm\n",
    "        t3 = t2 * t_norm\n",
    "        h00 = 2 * t3 - 3 * t2 + 1\n",
    "        h10 = t3 - 2 * t2 + t_norm\n",
    "        h01 = -2 * t3 + 3 * t2\n",
    "        h11 = t3 - t2\n",
    "        dx_segment = x1 - x0\n",
    "        return h00 * y0 + h10 * dx_segment * d0 + h01 * y1 + h11 * dx_segment * d1\n",
    "\n",
    "class GradientSMPA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.05, epochs=100, random_state=7, verbose=False,\n",
    "                 lambda_reg=0.0001, patience=10, decay_factor=0.9, min_learning_rate=1e-6,\n",
    "                 n_control_points=6, device=None, track_history=False, optimizer_type='adam',\n",
    "                 scheduler_type='reduce_on_plateau'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.track_history = track_history\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.scheduler_type = scheduler_type\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def _to_tensor(self, data, dtype=torch.float32):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.to(self.device, dtype=dtype, non_blocking=True)\n",
    "        return torch.tensor(data, dtype=dtype, device=self.device)\n",
    "\n",
    "    def _calculate_class_means(self, X, y):\n",
    "        mask_1 = y == 1\n",
    "        self.m1 = torch.mean(X[mask_1], dim=0)\n",
    "        self.m0 = torch.mean(X[~mask_1], dim=0)\n",
    "\n",
    "    def _initialize_control_points(self, X):\n",
    "        n_features = X.shape[1] - 1\n",
    "        self.spline_models = nn.ModuleList()\n",
    "        for i in range(n_features):\n",
    "            x_min, x_max = X[:, i].min().item(), X[:, i].max().item()\n",
    "            y_min, y_max = X[:, -1].min().item(), X[:, -1].max().item()\n",
    "            y_mid = (self.m0[-1] + self.m1[-1]) / 2\n",
    "            y_range = y_max - y_min\n",
    "            control_y = torch.empty(self.n_control_points, device=self.device).uniform_(\n",
    "                y_mid - y_range * 0.05, y_mid + y_range * 0.05\n",
    "            )\n",
    "            spline = OptimizedDifferentiablePchip(x_min, x_max, control_y, self.n_control_points).to(self.device)\n",
    "            self.spline_models.append(spline)\n",
    "        self.initial_control_points = [(m.x.clone() if m.x is not None else torch.linspace(x_min, x_max, self.n_control_points), m.y.clone()) for m in self.spline_models]\n",
    "\n",
    "    def _calculate_displacement(self, X):\n",
    "        total_spline = sum(spline(X[:, i]) for i, spline in enumerate(self.spline_models))\n",
    "        return X[:, -1] - total_spline\n",
    "\n",
    "    def _update_pseudo_labels(self, X, y):\n",
    "        m1_displacement = self._calculate_displacement(self.m1.unsqueeze(0))[0]\n",
    "        self.class_1_pseudo = 1 if m1_displacement > 0 else -1\n",
    "        self.class_0_pseudo = -self.class_1_pseudo\n",
    "        return torch.where(y == 1, self.class_1_pseudo, self.class_0_pseudo)\n",
    "\n",
    "    def _create_optimizer_and_scheduler(self):\n",
    "        params = [p for spline in self.spline_models for p in spline.parameters()]\n",
    "        if self.optimizer_type.lower() == 'adam':\n",
    "            optimizer = torch.optim.Adam(params, lr=self.initial_learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(params, lr=self.initial_learning_rate)\n",
    "        if self.scheduler_type.lower() == 'reduce_on_plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=self.decay_factor,\n",
    "                patience=self.patience, min_lr=self.min_learning_rate)\n",
    "        else:\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=self.patience, gamma=self.decay_factor\n",
    "            )\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            l = np.unique(y)\n",
    "            if len(l) != 2:\n",
    "                raise ValueError(\"Algorithm for binary classification only.\")\n",
    "            if X.shape[1] < 2:\n",
    "                raise ValueError(\"At least 2 features required\")\n",
    "\n",
    "            self.label_mapping = {l[0]: 0, l[1]: 1}\n",
    "            y = np.where(y == l[0], 0, 1)\n",
    "\n",
    "            X_tensor = self._to_tensor(X)\n",
    "            y_tensor = self._to_tensor(y, dtype=torch.long)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self._calculate_class_means(X_tensor, y_tensor)\n",
    "                self._initialize_control_points(X_tensor)\n",
    "\n",
    "            optimizer, scheduler = self._create_optimizer_and_scheduler()\n",
    "\n",
    "            best_error = float('inf')\n",
    "            best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "            best_class_1_pseudo = None\n",
    "\n",
    "            if self.track_history:\n",
    "                self.error_history_ = []\n",
    "                self.control_point_history = [self.initial_control_points]\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                for spline in self.spline_models:\n",
    "                    spline.update_spline()\n",
    "\n",
    "                total_spline = sum(spline(X_tensor[:, i]) for i, spline in enumerate(self.spline_models))\n",
    "                displacements = X_tensor[:, -1] - total_spline\n",
    "\n",
    "                pseudo_labels = self._update_pseudo_labels(X_tensor, y_tensor)\n",
    "                errors = displacements * pseudo_labels <= 0\n",
    "                error_count = errors.sum().item()\n",
    "\n",
    "                if self.verbose and epoch % 5 == 0:\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch}: Errors = {error_count}, LR = {current_lr:.6f}\")\n",
    "\n",
    "                if error_count < best_error:\n",
    "                    best_error = error_count\n",
    "                    best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "                    best_class_1_pseudo = self.class_1_pseudo\n",
    "                    self.best_epoch = epoch\n",
    "                    if error_count == 0 and epoch > 10:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Perfect separation achieved at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "                if self.track_history:\n",
    "                    self.error_history_.append(error_count)\n",
    "                    self.control_point_history.append(\n",
    "                        [(s.x.clone().cpu().detach().numpy(), s.y.clone().detach().cpu().numpy())\n",
    "                         for s in self.spline_models]\n",
    "                    )\n",
    "\n",
    "                if error_count == 0:\n",
    "                    continue\n",
    "\n",
    "                error_indices = torch.where(errors)[0]\n",
    "                displacements_err = displacements[error_indices]\n",
    "                y_err = y_tensor[error_indices]\n",
    "                ti = torch.where(y_err == 1, 1, -1)\n",
    "                loss = torch.mean(torch.relu(1.0 - ti * self.class_1_pseudo * displacements_err))\n",
    "\n",
    "                if self.lambda_reg > 0:\n",
    "                    smoothness_penalty = 0\n",
    "                    for spline in self.spline_models:\n",
    "                        y_diff = spline.y[1:] - spline.y[:-1]\n",
    "                        x_diff = spline.x[1:] - spline.x[:-1]\n",
    "                        smoothness_penalty += torch.mean((y_diff / (x_diff + 1e-8))**2)\n",
    "                    loss += self.lambda_reg * smoothness_penalty\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(error_count)\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    if optimizer.param_groups[0]['lr'] <= self.min_learning_rate:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Minimum learning rate reached at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            for spline, best_y in zip(self.spline_models, best_control_ys):\n",
    "                spline.y.data = best_y\n",
    "            self.class_1_pseudo = best_class_1_pseudo\n",
    "        except Exception as e:\n",
    "            print(f\"Error in SMPA fit: {str(e)}\", flush=True)\n",
    "            import traceback\n",
    "            traceback.print_exc(flush=True)\n",
    "            raise\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        predictions = torch.where(displacements > 0,\n",
    "                                  torch.tensor(1 if self.class_1_pseudo > 0 else 0, device=self.device),\n",
    "                                  torch.tensor(0 if self.class_1_pseudo > 0 else 1, device=self.device))\n",
    "        pred_numpy = predictions.cpu().numpy()\n",
    "        reverse_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "        return np.array([reverse_mapping[p] for p in pred_numpy])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        raw_probs = 1 / (1 + torch.exp(-displacements * 0.5))\n",
    "        probs = torch.zeros(X.shape[0], 2, device=self.device)\n",
    "        if self.class_1_pseudo > 0:\n",
    "            probs[:, 1] = raw_probs\n",
    "            probs[:, 0] = 1 - raw_probs\n",
    "        else:\n",
    "            probs[:, 0] = raw_probs\n",
    "            probs[:, 1] = 1 - raw_probs\n",
    "        return probs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f94c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SMPATransformer(nn.Module):\n",
    "    def __init__(self, n_features, n_control_points=6, lambda_reg=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features - 1\n",
    "        self.n_control_points = n_control_points\n",
    "        # Convert lambda_reg to float to handle string inputs\n",
    "        try:\n",
    "            self.lambda_reg = float(lambda_reg)\n",
    "        except (TypeError, ValueError) as e:\n",
    "            raise ValueError(f\"lambda_reg must be convertible to float, got {lambda_reg} ({type(lambda_reg)})\") from e\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.spline_models = nn.ModuleList()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def initialize_from_smpa(self, smpa, X=None):\n",
    "        self.spline_models = nn.ModuleList()\n",
    "        for i in range(self.n_features):\n",
    "            original_spline = smpa.spline_models[i]\n",
    "            original_spline.update_spline()\n",
    "            control_x = original_spline.x.detach().cpu().numpy()\n",
    "            control_y = original_spline.y.detach().cpu().numpy()\n",
    "            control_theta = original_spline.theta.detach().cpu().numpy()\n",
    "            if X is not None:\n",
    "                x_min, x_max = float(X[:, i].min()), float(X[:, i].max())\n",
    "            else:\n",
    "                x_min, x_max = float(original_spline.x_min), float(original_spline.x_max)\n",
    "            x_tensor = torch.tensor(control_x, device=self.device, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(control_y, device=self.device, dtype=torch.float32)\n",
    "            theta_tensor = torch.tensor(control_theta, device=self.device, dtype=torch.float32)\n",
    "            spline = OptimizedDifferentiablePchip(x_min=x_min, x_max=x_max,\n",
    "                                                 y_init=y_tensor, n_control_points=self.n_control_points)\n",
    "            spline.x = x_tensor\n",
    "            spline.y = nn.Parameter(y_tensor, requires_grad=False)\n",
    "            spline.theta = nn.Parameter(theta_tensor, requires_grad=False)\n",
    "            spline.update_spline()\n",
    "            self.spline_models.append(spline)\n",
    "\n",
    "    def _to_tensor(self, data, dtype=torch.float32):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.to(self.device, dtype=dtype, non_blocking=True)\n",
    "        return torch.tensor(data, dtype=dtype, device=self.device)\n",
    "\n",
    "    def _compute_linear_function(self, spline, X):\n",
    "        \"\"\"Compute linear function from spline's first and last control points.\"\"\"\n",
    "        x0, x1 = spline.x[0], spline.x[-1]\n",
    "        y0, y1 = spline.y[0], spline.y[-1]\n",
    "        m = (y1 - y0) / (x1 - x0 + 1e-8)  # Slope\n",
    "        b = y0 - m * x0  # Intercept\n",
    "        y_linear = m * X + b\n",
    "        return y_linear\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self._to_tensor(X)\n",
    "        total_adjustment = torch.zeros(X.shape[0], device=self.device)\n",
    "        for i, spline in enumerate(self.spline_models):\n",
    "            spline_output = spline(X[:, i])\n",
    "            y_linear = self._compute_linear_function(spline, X[:, i])\n",
    "            # Compute regularization factor based on smoothness penalty\n",
    "            y_diff = spline.y[1:] - spline.y[:-1]\n",
    "            x_diff = spline.x[1:] - spline.x[:-1]\n",
    "            penalty = torch.mean((y_diff / (x_diff + 1e-8))**2)\n",
    "            lambda_reg = torch.tensor(self.lambda_reg, device=self.device, dtype=torch.float32)\n",
    "            reg_factor = 1 / (1 + lambda_reg * penalty)\n",
    "            total_adjustment += reg_factor * (spline_output - y_linear)\n",
    "        Z = X.clone()\n",
    "        Z[:, -1] = X[:, -1] - total_adjustment\n",
    "        return Z\n",
    "\n",
    "    def compute_smoothness_penalty(self):\n",
    "        smoothness_penalty = 0\n",
    "        for spline in self.spline_models:\n",
    "            y_diff = spline.y[1:] - spline.y[:-1]\n",
    "            x_diff = spline.x[1:] - spline.x[:-1]\n",
    "            penalty = torch.mean((y_diff / (x_diff + 1e-8))**2)\n",
    "            smoothness_penalty += penalty\n",
    "        return smoothness_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3144f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMPAEndtoEnd(nn.Module):\n",
    "    def __init__(self, n_features, n_control_points=6, classifier_type='linear', hidden_dim=16, device=None):\n",
    "        super().__init__()\n",
    "        self.transformer = SMPATransformer(n_features, n_control_points, 0.0001, device)\n",
    "        if classifier_type == 'linear':\n",
    "            self.classifier = nn.Linear(n_features, 1)\n",
    "        elif classifier_type == 'mlp':\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(n_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Classifier must be 'linear' or 'mlp'\")\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def initialize_from_smpa(self, smpa, X=None):\n",
    "        self.transformer.initialize_from_smpa(smpa, X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.transformer(X).detach()  # Approach 2: Fixed splines\n",
    "        logits = self.classifier(Z)\n",
    "        return logits\n",
    "\n",
    "    def compute_smoothness_penalty(self):\n",
    "        return self.transformer.compute_smoothness_penalty()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logits = self.forward(X)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return torch.cat([1 - probs, probs], dim=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs[:, 1] > 0.5).long()\n",
    "\n",
    "def train_smpa_end_to_end(X, y, n_control_points=6, classifier_type='linear', smpa_epochs=200,\n",
    "                         finetune_epochs=100, learning_rate=0.05, batch_size=32, verbose=False):\n",
    "    smpa = GradientSMPA(\n",
    "        n_control_points=n_control_points,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=smpa_epochs,\n",
    "        verbose=verbose,\n",
    "        random_state=7\n",
    "    )\n",
    "    smpa.fit(X, y)\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    model = SMPAEndtoEnd(\n",
    "        n_features=n_features,\n",
    "        n_control_points=n_control_points,\n",
    "        classifier_type=classifier_type,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    model.initialize_from_smpa(smpa, X=X)\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32, device=model.device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32, device=model.device).view(-1, 1)\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(finetune_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        epoch_loss /= len(X_tensor)\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Fine-tune Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, smpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa96c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61518/506893241.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sonar_df = sonar_df.replace({'label':dict1})\n"
     ]
    }
   ],
   "source": [
    "sonar_df = pd.read_csv(\"./Dataset_Benchmarking/Datasets/sonar.csv\", header = None)\n",
    "num_features = sonar_df.shape[1] - 1  # Assuming the last column is the label\n",
    "feature_names = [f'feature{i+1}' for i in range(num_features)] + ['label']\n",
    "sonar_df.columns = feature_names\n",
    "dict1 = {'R':0,'M':1}\n",
    "sonar_df = sonar_df.replace({'label':dict1})\n",
    "sonar_df['label'] = sonar_df['label'].apply(pd.to_numeric, errors = 'coerce')\n",
    "sonar_df = shuffle(sonar_df, random_state=42)\n",
    "\n",
    "train_df, test_df = train_test_split(sonar_df, test_size = 0.2, random_state = 22)\n",
    "X_train = train_df.drop(\"label\", axis = 1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_test = test_df.drop(\"label\", axis = 1)\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "133132f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature52</th>\n",
       "      <th>feature53</th>\n",
       "      <th>feature54</th>\n",
       "      <th>feature55</th>\n",
       "      <th>feature56</th>\n",
       "      <th>feature57</th>\n",
       "      <th>feature58</th>\n",
       "      <th>feature59</th>\n",
       "      <th>feature60</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0972</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.3127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0786</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "161    0.0305    0.0363    0.0214    0.0227    0.0456    0.0665    0.0939   \n",
       "15     0.0298    0.0615    0.0650    0.0921    0.1615    0.2294    0.2176   \n",
       "73     0.0139    0.0222    0.0089    0.0108    0.0215    0.0136    0.0659   \n",
       "96     0.0181    0.0146    0.0026    0.0141    0.0421    0.0473    0.0361   \n",
       "166    0.0411    0.0277    0.0604    0.0525    0.0489    0.0385    0.0611   \n",
       "\n",
       "     feature8  feature9  feature10  ...  feature52  feature53  feature54  \\\n",
       "161    0.0972    0.2535     0.3127  ...     0.0200     0.0070     0.0070   \n",
       "15     0.2033    0.1459     0.0852  ...     0.0031     0.0153     0.0071   \n",
       "73     0.0954    0.0786     0.1015  ...     0.0062     0.0072     0.0113   \n",
       "96     0.0741    0.1398     0.1045  ...     0.0223     0.0255     0.0145   \n",
       "166    0.1117    0.1237     0.2300  ...     0.0217     0.0038     0.0019   \n",
       "\n",
       "     feature55  feature56  feature57  feature58  feature59  feature60  label  \n",
       "161     0.0086     0.0089     0.0074     0.0042     0.0055     0.0021      1  \n",
       "15      0.0212     0.0076     0.0152     0.0049     0.0200     0.0073      0  \n",
       "73      0.0012     0.0022     0.0025     0.0059     0.0039     0.0048      0  \n",
       "96      0.0233     0.0041     0.0018     0.0048     0.0089     0.0085      0  \n",
       "166     0.0065     0.0132     0.0108     0.0050     0.0085     0.0044      1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5a6d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891723ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Errors = 74, LR = 0.400000\n",
      "Epoch 5: Errors = 74, LR = 0.400000\n",
      "Epoch 10: Errors = 69, LR = 0.400000\n",
      "Epoch 15: Errors = 18, LR = 0.400000\n",
      "Epoch 20: Errors = 73, LR = 0.400000\n",
      "Epoch 25: Errors = 77, LR = 0.400000\n",
      "Epoch 30: Errors = 21, LR = 0.360000\n",
      "Epoch 35: Errors = 144, LR = 0.360000\n",
      "Epoch 40: Errors = 74, LR = 0.324000\n",
      "Epoch 45: Errors = 80, LR = 0.324000\n",
      "Epoch 50: Errors = 25, LR = 0.291600\n",
      "Epoch 55: Errors = 83, LR = 0.291600\n",
      "Epoch 60: Errors = 74, LR = 0.291600\n",
      "Epoch 65: Errors = 75, LR = 0.262440\n",
      "Epoch 70: Errors = 66, LR = 0.262440\n",
      "Epoch 75: Errors = 64, LR = 0.262440\n",
      "Epoch 80: Errors = 74, LR = 0.236196\n",
      "Epoch 85: Errors = 71, LR = 0.236196\n",
      "Epoch 90: Errors = 71, LR = 0.212576\n",
      "Epoch 95: Errors = 74, LR = 0.212576\n",
      "Epoch 100: Errors = 77, LR = 0.191319\n",
      "Epoch 105: Errors = 43, LR = 0.191319\n",
      "Epoch 110: Errors = 24, LR = 0.172187\n",
      "Perfect separation achieved at epoch 112\n",
      "Fine-tune Epoch 0, Loss: 1.3888\n",
      "Fine-tune Epoch 10, Loss: 0.0319\n",
      "Fine-tune Epoch 20, Loss: 0.0226\n",
      "Fine-tune Epoch 30, Loss: 0.0193\n",
      "Fine-tune Epoch 40, Loss: 0.0122\n",
      "Fine-tune Epoch 50, Loss: 0.0086\n",
      "Fine-tune Epoch 60, Loss: 0.0069\n",
      "Fine-tune Epoch 70, Loss: 0.0060\n",
      "Fine-tune Epoch 80, Loss: 0.0051\n",
      "Fine-tune Epoch 90, Loss: 0.0047\n"
     ]
    }
   ],
   "source": [
    "model, smpa = train_smpa_end_to_end(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_control_points=6,\n",
    "    classifier_type='linear',\n",
    "    smpa_epochs=350,\n",
    "    finetune_epochs=100,\n",
    "    learning_rate=0.4,\n",
    "    batch_size=32,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10a5a457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMPATransformer(\n",
       "  (spline_models): ModuleList(\n",
       "    (0-58): 59 x OptimizedDifferentiablePchip()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize transformer for visualization\n",
    "transformer = SMPATransformer(n_features=X_train.shape[1], n_control_points=15)\n",
    "transformer.initialize_from_smpa(smpa, X=X_test)\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1da7d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy of end-to-end model: 1.0\n",
      "Test Accuracy of end-to-end model: 0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "# Evaluate training\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=transformer.device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long, device=transformer.device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model.predict(X_train_tensor).cpu().numpy()\n",
    "print(\"Train Accuracy of end-to-end model:\", (y_pred == y_train).mean())\n",
    "\n",
    "# Evaluate testing\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=transformer.device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long, device=transformer.device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model.predict(X_test_tensor).cpu().numpy()\n",
    "print(\"Test Accuracy of end-to-end model:\", (y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eed14870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smpa.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "903a00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def extract_transformed_features(model, X, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extracts the transformed feature space coordinates from an SMPAEndtoEnd model as a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        model (SMPAEndtoEnd): Trained SMPAEndtoEnd model.\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        batch_size (int): Batch size for processing data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Transformed coordinates of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    transformed_features = []\n",
    "    with torch.no_grad():\n",
    "        for [X_batch] in loader:\n",
    "            Z = model.transformer(X_batch).detach()\n",
    "            transformed_features.append(Z.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(transformed_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c90d5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cumulate_transformed_features_with_final_dataset(X_train, y_train, X_test, y_test, n_control_points=15,\n",
    "                                                             classifier_type='linear', smpa_epochs=200,\n",
    "                                                             finetune_epochs=150, learning_rate=0.1,\n",
    "                                                             batch_size=32, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains SMPAEndtoEnd with each feature as the last feature, transforms it, uses transformed features\n",
    "    in subsequent runs, and returns results plus the final fully transformed dataset.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data of shape (n_samples, n_features).\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        X_test (np.ndarray): Test data.\n",
    "        y_test (np.ndarray): Test labels.\n",
    "        n_control_points (int): Number of control points for SMPA.\n",
    "        classifier_type (str): 'linear' or 'mlp' for classifier head.\n",
    "        smpa_epochs (int): Epochs for initial SMPA training.\n",
    "        finetune_epochs (int): Epochs for fine-tuning end-to-end model.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        batch_size (int): Batch size for training.\n",
    "        verbose (bool): If True, print training progress.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results, X_train_final, X_test_final)\n",
    "            - results: List of dicts with last_feature_index, train_accuracy, test_accuracy, transformed_features, current_dataset.\n",
    "            - X_train_final: Fully transformed training dataset (np.ndarray).\n",
    "            - X_test_final: Fully transformed test dataset (np.ndarray).\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    results = []\n",
    "\n",
    "    # Working copies of datasets\n",
    "    X_train_current = X_train.copy()\n",
    "    X_test_current = X_test.copy()\n",
    "\n",
    "    def train_model(X, y, X_t, y_t):\n",
    "        model, smpa = train_smpa_end_to_end(X, y, n_control_points, classifier_type,\n",
    "                                           smpa_epochs, finetune_epochs, learning_rate,\n",
    "                                           batch_size, verbose)\n",
    "        Z = extract_transformed_features(model, X, batch_size)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "            y_pred_train = model.predict(X_tensor).cpu().numpy()\n",
    "            X_test_tensor = torch.tensor(X_t, dtype=torch.float32, device=device)\n",
    "            y_pred_test = model.predict(X_test_tensor).cpu().numpy()\n",
    "        train_acc = accuracy_score(y, y_pred_train)\n",
    "        test_acc = accuracy_score(y_t, y_pred_test)\n",
    "        return model, Z, train_acc, test_acc\n",
    "\n",
    "    # Iterate through each feature as the last feature\n",
    "    for i in range(n_features):\n",
    "        # Reorder features: move feature i to last position\n",
    "        feature_order = list(range(n_features))\n",
    "        feature_order.pop(i)\n",
    "        feature_order.append(i)  # [0, 1, ..., i-1, i+1, ..., n-1, i]\n",
    "        X_train_reordered = X_train_current[:, feature_order]\n",
    "        X_test_reordered = X_test_current[:, feature_order]\n",
    "\n",
    "        # Train and evaluate\n",
    "        model, Z, train_acc, test_acc = train_model(X_train_reordered, y_train, X_test_reordered, y_test)\n",
    "\n",
    "        # Update the dataset: replace feature i with its transformed version\n",
    "        X_train_current[:, i] = Z[:, i]  # Transformed last feature (index i in original order)\n",
    "        X_test_current[:, i] = extract_transformed_features(model, X_test_reordered, batch_size)[:, i]\n",
    "\n",
    "        results.append({\n",
    "            'last_feature_index': i,\n",
    "            'train_accuracy': train_acc,\n",
    "            'test_accuracy': test_acc,\n",
    "            'transformed_features': Z,\n",
    "            'current_dataset': X_train_current.copy()\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"Transformed Feature {i}: Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Return results and final transformed datasets\n",
    "    return results, X_train_current, X_test_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e5b4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Feature 0: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 1: Train 1.0000, Test 0.8810\n",
      "Transformed Feature 2: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 3: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 4: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 5: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 6: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 7: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 8: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 9: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 10: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 11: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 12: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 13: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 14: Train 1.0000, Test 0.8571\n",
      "Transformed Feature 15: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 16: Train 1.0000, Test 0.8810\n",
      "Transformed Feature 17: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 18: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 19: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 20: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 21: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 22: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 23: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 24: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 25: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 26: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 27: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 28: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 29: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 30: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 31: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 32: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 33: Train 1.0000, Test 0.8571\n",
      "Transformed Feature 34: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 35: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 36: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 37: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 38: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 39: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 40: Train 1.0000, Test 0.8571\n",
      "Transformed Feature 41: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 42: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 43: Train 1.0000, Test 0.8810\n",
      "Transformed Feature 44: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 45: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 46: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 47: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 48: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 49: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 50: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 51: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 52: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 53: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 54: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 55: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 56: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 57: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 58: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 59: Train 1.0000, Test 0.6429\n"
     ]
    }
   ],
   "source": [
    "results, X_train_final, X_test_final = train_and_cumulate_transformed_features_with_final_dataset(\n",
    "    X_train, y_train, X_test, y_test, classifier_type='linear'\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"Transformed Feature {res['last_feature_index']}: Train {res['train_accuracy']:.4f}, Test {res['test_accuracy']:.4f}\")\n",
    "# Use X_train_final, X_test_final for second pass:\n",
    "results2, X_train_final2, X_test_final2 = train_and_cumulate_transformed_features_with_final_dataset(\n",
    "     X_train_final, y_train, X_test_final, y_test, classifier_type='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e40d9062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Feature 0: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 1: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 2: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 3: Train 1.0000, Test 0.6190\n",
      "Transformed Feature 4: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 5: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 6: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 7: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 8: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 9: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 10: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 11: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 12: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 13: Train 1.0000, Test 0.8333\n",
      "Transformed Feature 14: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 15: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 16: Train 1.0000, Test 0.6429\n",
      "Transformed Feature 17: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 18: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 19: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 20: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 21: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 22: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 23: Train 1.0000, Test 0.7857\n",
      "Transformed Feature 24: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 25: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 26: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 27: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 28: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 29: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 30: Train 1.0000, Test 0.6429\n",
      "Transformed Feature 31: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 32: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 33: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 34: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 35: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 36: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 37: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 38: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 39: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 40: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 41: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 42: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 43: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 44: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 45: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 46: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 47: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 48: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 49: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 50: Train 1.0000, Test 0.8095\n",
      "Transformed Feature 51: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 52: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 53: Train 1.0000, Test 0.7143\n",
      "Transformed Feature 54: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 55: Train 1.0000, Test 0.7619\n",
      "Transformed Feature 56: Train 1.0000, Test 0.7381\n",
      "Transformed Feature 57: Train 1.0000, Test 0.6905\n",
      "Transformed Feature 58: Train 1.0000, Test 0.6667\n",
      "Transformed Feature 59: Train 1.0000, Test 0.7143\n"
     ]
    }
   ],
   "source": [
    "for res in results2:\n",
    "    print(f\"Transformed Feature {res['last_feature_index']}: Train {res['train_accuracy']:.4f}, Test {res['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c4b360",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results3, X_train_final3, X_test_final3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_cumulate_transformed_features_with_final_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43mX_train_final2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_final2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results3:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformed Feature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_feature_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mtrain_and_cumulate_transformed_features_with_final_dataset\u001b[0;34m(X_train, y_train, X_test, y_test, n_control_points, classifier_type, smpa_epochs, finetune_epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m X_test_reordered \u001b[38;5;241m=\u001b[39m X_test_current[:, feature_order]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model, Z, train_acc, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_reordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Update the dataset: replace feature i with its transformed version\u001b[39;00m\n\u001b[1;32m     64\u001b[0m X_train_current[:, i] \u001b[38;5;241m=\u001b[39m Z[:, i]  \u001b[38;5;66;03m# Transformed last feature (index i in original order)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mtrain_and_cumulate_transformed_features_with_final_dataset.<locals>.train_model\u001b[0;34m(X, y, X_t, y_t)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X, y, X_t, y_t):\n\u001b[0;32m---> 37\u001b[0m     model, smpa \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_smpa_end_to_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_control_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msmpa_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     Z \u001b[38;5;241m=\u001b[39m extract_transformed_features(model, X, batch_size)\n\u001b[1;32m     41\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m, in \u001b[0;36mtrain_smpa_end_to_end\u001b[0;34m(X, y, n_control_points, classifier_type, smpa_epochs, finetune_epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     71\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 72\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, y_batch)\n\u001b[1;32m     74\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mSMPAEndtoEnd.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Approach 2: Fixed splines\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(Z)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mSMPATransformer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     58\u001b[0m total_adjustment \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, spline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspline_models):\n\u001b[0;32m---> 60\u001b[0m     spline_output \u001b[38;5;241m=\u001b[39m \u001b[43mspline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     y_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_linear_function(spline, X[:, i])\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Compute regularization factor based on smoothness penalty\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results3, X_train_final3, X_test_final3 = train_and_cumulate_transformed_features_with_final_dataset(\n",
    "     X_train_final2, y_train, X_test_final2, y_test, classifier_type='linear')\n",
    "\n",
    "for res in results3:\n",
    "    print(f\"Transformed Feature {res['last_feature_index']}: Train {res['train_accuracy']:.4f}, Test {res['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Feature 0: Train 0.6417, Test 0.6948\n",
      "Transformed Feature 1: Train 0.6221, Test 0.5325\n",
      "Transformed Feature 2: Train 0.6450, Test 0.5584\n",
      "Transformed Feature 3: Train 0.6629, Test 0.6948\n",
      "Transformed Feature 4: Train 0.6433, Test 0.6948\n",
      "Transformed Feature 5: Train 0.6629, Test 0.6948\n",
      "Transformed Feature 6: Train 0.6596, Test 0.6299\n",
      "Transformed Feature 7: Train 0.6287, Test 0.5130\n"
     ]
    }
   ],
   "source": [
    "results4, X_train_final4, X_test_final4 = train_and_cumulate_transformed_features_with_final_dataset(\n",
    "     X_train_final3, y_train, X_test_final3, y_test, classifier_type='linear')\n",
    "\n",
    "for res in results4:\n",
    "    print(f\"Transformed Feature {res['last_feature_index']}: Train {res['train_accuracy']:.4f}, Test {res['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29795a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMPA Training accuracy on transformed data: 0.36156351791530944\n",
      "SMPA Testing accuracy on transformed data: 0.2987012987012987\n"
     ]
    }
   ],
   "source": [
    "smpa1 = GradientSMPA(n_control_points=15, learning_rate=0.1, epochs=350,lambda_reg=0.0001)\n",
    "smpa1.fit(X_train_final, y_train)\n",
    "print(f\"SMPA Training accuracy on transformed data: {smpa1.score(X_train_final,y_train)}\")\n",
    "print(f\"SMPA Testing accuracy on transformed data: {smpa1.score(X_test_final,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b57aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Standalone SVM on the original dataset'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Standalone SVM on the original dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d599b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "613679e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform grid search for SVM\n",
    "def svm_grid_search(X_train, y_train):\n",
    "    # Create a pipeline with scaling and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(probability=True))\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'svm__C': [0.1, 1, 10, 100],\n",
    "        'svm__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'svm__kernel': ['rbf', 'sigmoid']\n",
    "    }\n",
    "\n",
    "    # Initialize grid search\n",
    "    print(\"Starting Grid Search...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        verbose=3,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Perform grid search\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Grid search completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Return the best estimator\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to evaluate SVM performance\n",
    "def evaluate_svm(model, X_train, y_train, X_test, y_test):\n",
    "    # Training performance\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    # Test performance\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"SVM Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"SVM Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "# Function to plot decision boundary\n",
    "def plot_svm_decision_boundary(model, X, y):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # Get predictions for all mesh points\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    # Plot the training points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.colorbar(scatter)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"SVM Decision Boundary\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9b2af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.676 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.676 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.559 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=rbf;, score=0.559 total time=   0.0s[CV 2/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.559 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.758 total time=   0.0s[CV 3/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=1, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=1, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=0.1, svm__gamma=1, svm__kernel=sigmoid;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END svm__C=0.1, svm__gamma=1, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 3/5] END svm__C=0.1, svm__gamma=1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=1, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=scale, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=scale, svm__kernel=rbf;, score=0.882 total time=   0.0s\n",
      "[CV 5/5] END svm__C=0.1, svm__gamma=1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=scale, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=scale, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=scale, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.853 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=auto, svm__kernel=rbf;, score=0.882 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END svm__C=0.1, svm__gamma=auto, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=auto, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=auto, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=auto, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.001, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.001, svm__kernel=rbf;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.001, svm__kernel=rbf;, score=0.765 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.606 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.01, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=auto, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.01, svm__kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.853 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.001, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.01, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.001, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.01, svm__kernel=rbf;, score=0.727 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.01, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.818 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.1, svm__kernel=rbf;, score=0.882 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.1, svm__kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.1, svm__kernel=rbf;, score=0.727 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.515 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.485 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=1, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.618 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=1, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=1, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=scale, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=scale, svm__kernel=rbf;, score=0.941 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.606 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=auto, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=scale, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 5/5] END svm__C=1, svm__gamma=1, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=auto, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END svm__C=1, svm__gamma=1, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=1, svm__gamma=1, svm__kernel=sigmoid;, score=0.529 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.001, svm__kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 2/5] END svm__C=1, svm__gamma=1, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=scale, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 4/5] END svm__C=1, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.001, svm__kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=scale, svm__kernel=sigmoid;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=auto, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=auto, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.001, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=auto, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=scale, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.01, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=scale, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.001, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=auto, svm__kernel=rbf;, score=0.941 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.01, svm__kernel=rbf;, score=0.882 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.818 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=scale, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=scale, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.01, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.001, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.01, svm__kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.1, svm__kernel=rbf;, score=0.853 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.515 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=auto, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=auto, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.559 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.606 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.01, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=1, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=auto, svm__kernel=sigmoid;, score=0.765 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.606 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=scale, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=1, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=1, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=auto, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=scale, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 3/5] END svm__C=10, svm__gamma=0.1, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=auto, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=scale, svm__kernel=rbf;, score=0.941 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=scale, svm__kernel=sigmoid;, score=0.735 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=scale, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=auto, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=scale, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=10, svm__gamma=1, svm__kernel=rbf;, score=0.576 total time=   0.0s[CV 3/5] END svm__C=100, svm__gamma=auto, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.001, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.001, svm__kernel=rbf;, score=0.824 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=scale, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 4/5] END svm__C=10, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 5/5] END svm__C=10, svm__gamma=1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=scale, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END svm__C=10, svm__gamma=1, svm__kernel=sigmoid;, score=0.500 total time=   0.0s[CV 1/5] END svm__C=100, svm__gamma=auto, svm__kernel=rbf;, score=0.941 total time=   0.0s\n",
      "\n",
      "[CV 4/5] END svm__C=100, svm__gamma=auto, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.765 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.001, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=auto, svm__kernel=rbf;, score=0.818 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=scale, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.606 total time=   0.0s[CV 3/5] END svm__C=100, svm__gamma=0.01, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=auto, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=auto, svm__kernel=rbf;, score=0.909 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.758 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=auto, svm__kernel=sigmoid;, score=0.735 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=scale, svm__kernel=rbf;, score=0.848 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=0.001, svm__kernel=rbf;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.01, svm__kernel=rbf;, score=0.882 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.01, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.1, svm__kernel=rbf;, score=0.853 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=0.001, svm__kernel=sigmoid;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=0.1, svm__kernel=rbf;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.1, svm__kernel=rbf;, score=0.758 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.545 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.001, svm__kernel=rbf;, score=0.727 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.485 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=0.01, svm__kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.1, svm__kernel=sigmoid;, score=0.606 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=0.01, svm__kernel=rbf;, score=0.788 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=1, svm__kernel=rbf;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=1, svm__kernel=sigmoid;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=1, svm__kernel=sigmoid;, score=0.515 total time=   0.0s\n",
      "[CV 3/5] END svm__C=100, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=1, svm__kernel=sigmoid;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 4/5] END svm__C=100, svm__gamma=1, svm__kernel=sigmoid;, score=0.576 total time=   0.0s\n",
      "[CV 2/5] END svm__C=100, svm__gamma=1, svm__kernel=rbf;, score=0.576 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=1, svm__kernel=rbf;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END svm__C=100, svm__gamma=0.01, svm__kernel=sigmoid;, score=0.765 total time=   0.0s\n",
      "[CV 5/5] END svm__C=100, svm__gamma=1, svm__kernel=sigmoid;, score=0.636 total time=   0.0s\n",
      "Grid search completed in 1.99 seconds\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Best cross-validation score: 0.8731\n",
      "SVM Train Accuracy: 100.00%\n",
      "SVM Test Accuracy: 90.48%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91        23\n",
      "           1       0.86      0.95      0.90        19\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.90      0.91      0.90        42\n",
      "weighted avg       0.91      0.90      0.90        42\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9047619047619048)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm = svm_grid_search(X_train, y_train)\n",
    "evaluate_svm(best_svm, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c72d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFF = really fucking fast :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.interpolate import PchipInterpolator, CubicSpline\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class SMPAEstimator(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.005, epochs=50, random_state=None, verbose=False,\n",
    "                 lambda_scaling='log', patience=5, decay_factor=0.5, min_learning_rate=1e-6,\n",
    "                 n_control_points=5, smoothing_factor=0.1, spline_type='pchip', early_stop_patience=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_scaling = lambda_scaling\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "\n",
    "        if lambda_scaling not in ['log', 'sqrt', 'none']:\n",
    "            raise ValueError(\"lambda_scaling must be one of 'log', 'sqrt', or 'none'\")\n",
    "        if spline_type not in ['cubic', 'pchip']:\n",
    "            raise ValueError(\"spline_type must be one of 'cubic' or 'pchip'\")\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _calculate_class_means(self, X, y):\n",
    "        mask_1 = y == 1\n",
    "        self.m1 = X[mask_1].mean(axis=0)\n",
    "        self.m0 = X[~mask_1].mean(axis=0)\n",
    "\n",
    "    def _initialize_control_points(self, X):\n",
    "        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "        y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "        x_range = x_max - x_min\n",
    "        y_range = y_max - y_min\n",
    "        x_min_extended = x_min - 0.2 * x_range\n",
    "        x_max_extended = x_max + 0.2 * x_range\n",
    "        self.control_x = np.linspace(x_min_extended, x_max_extended, self.n_control_points)\n",
    "        y_mid = (self.m0[1] + self.m1[1]) / 2\n",
    "        self.control_y = np.random.uniform(y_mid - y_range * 0.05, y_mid + y_range * 0.05, self.n_control_points)\n",
    "        if self.n_control_points > 2:\n",
    "            self.control_y[0] = self.control_y[1]\n",
    "            self.control_y[-1] = self.control_y[-2]\n",
    "\n",
    "    def _fit_spline(self):\n",
    "        if not np.all(np.diff(self.control_x) > 0):\n",
    "            raise ValueError(\"control_x must be strictly increasing for spline fitting\")\n",
    "        if self.spline_type == 'cubic':\n",
    "            self.spline = CubicSpline(self.control_x, self.control_y, bc_type='clamped')\n",
    "        else:\n",
    "            self.spline = PchipInterpolator(self.control_x, self.control_y)\n",
    "\n",
    "    def _calculate_displacement(self, X):\n",
    "        return X[:, 1] - self.spline(X[:, 0])\n",
    "\n",
    "    def _update_pseudo_labels(self, X, y):\n",
    "        m1_displacement = self._calculate_displacement(self.m1.reshape(1, -1))[0]\n",
    "        self.class_1_pseudo = 1 if m1_displacement > 0 else -1\n",
    "        self.class_0_pseudo = -self.class_1_pseudo\n",
    "        return np.where(y == 1, self.class_1_pseudo, self.class_0_pseudo)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if not set(np.unique(y)).issubset({0, 1}) or X.shape[1] != 2:\n",
    "            raise ValueError(\"Labels must be 0 and 1, and X must be 2D\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self._calculate_class_means(X, y)\n",
    "        self._initialize_control_points(X)\n",
    "        self._fit_spline()\n",
    "\n",
    "        best_error = float('inf')\n",
    "        best_control_x = None\n",
    "        best_control_y = None\n",
    "        best_class_1_pseudo = None\n",
    "        patience_counter = 0\n",
    "        early_stop_counter = 0\n",
    "        current_learning_rate = self.initial_learning_rate\n",
    "\n",
    "        indices_class_0 = np.where(y == 0)[0]\n",
    "        indices_class_1 = np.where(y == 1)[0]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            pseudo_labels = self._update_pseudo_labels(X, y)\n",
    "            displacements = self._calculate_displacement(X)\n",
    "            errors = (displacements * pseudo_labels <= 0)\n",
    "            error_count = np.sum(errors)\n",
    "\n",
    "            if self.verbose and epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: Errors = {error_count}, LR = {current_learning_rate:.6f}\")\n",
    "\n",
    "            if error_count < best_error:\n",
    "                best_error = error_count\n",
    "                best_control_x = self.control_x.copy()\n",
    "                best_control_y = self.control_y.copy()\n",
    "                best_class_1_pseudo = self.class_1_pseudo\n",
    "                patience_counter = 0\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                early_stop_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    current_learning_rate = max(current_learning_rate * self.decay_factor, self.min_learning_rate)\n",
    "                    patience_counter = 0\n",
    "                    if current_learning_rate == self.min_learning_rate:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Min learning rate reached at epoch {epoch}\")\n",
    "                        break\n",
    "                if early_stop_counter >= self.early_stop_patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            if errors.any():\n",
    "                error_indices = np.where(errors)[0]\n",
    "                np.random.shuffle(error_indices)\n",
    "\n",
    "                for idx in error_indices:\n",
    "                    d = X[idx]\n",
    "                    distances = np.abs(self.control_x - d[0])\n",
    "                    nearest_idx = np.argmin(distances)\n",
    "                    distance = distances[nearest_idx]\n",
    "\n",
    "                    lmbda = {'log': np.log1p(distance), 'sqrt': np.sqrt(distance), 'none': distance}[self.lambda_scaling]\n",
    "                    margin = max(0.1, min(1.0, lmbda * 0.2))\n",
    "\n",
    "                    opp_indices = indices_class_0 if y[idx] == 1 else indices_class_1\n",
    "                    opp_displacements = displacements[opp_indices]\n",
    "                    opp_labels = pseudo_labels[opp_indices]\n",
    "                    correct_opp = opp_indices[opp_displacements * opp_labels > margin]\n",
    "\n",
    "                    if len(correct_opp) > 0:\n",
    "                        n_random = max(1, min(len(correct_opp) // 5, 5))\n",
    "                        random_correct = np.random.choice(correct_opp, size=n_random, replace=False)\n",
    "                        random_avg_opp = np.mean(X[random_correct], axis=0)\n",
    "                        # Only update control_y, keep control_x fixed\n",
    "                        step_x = 0  # Lock control_x updates\n",
    "                        step_y = (random_avg_opp[1] - self.control_y[nearest_idx]) * current_learning_rate / (1 + lmbda)\n",
    "                    else:\n",
    "                        step_x = 0\n",
    "                        step_y = -pseudo_labels[idx] * margin * current_learning_rate\n",
    "\n",
    "                    # Apply updates iteratively\n",
    "                    self.control_x[nearest_idx] += step_x  # This will be 0, but keeping structure\n",
    "                    self.control_y[nearest_idx] += step_y\n",
    "\n",
    "                    # Boundary handling for control_y\n",
    "                    if self.n_control_points > 2:\n",
    "                        self.control_y[0] = self.control_y[1]\n",
    "                        self.control_y[-1] = self.control_y[-2]\n",
    "\n",
    "                    # Refit spline after each update\n",
    "                    self._fit_spline()\n",
    "                    pseudo_labels = self._update_pseudo_labels(X, y)\n",
    "                    displacements = self._calculate_displacement(X)\n",
    "\n",
    "        self.control_x = best_control_x\n",
    "        self.control_y = best_control_y\n",
    "        self._fit_spline()\n",
    "        self.class_1_pseudo = best_class_1_pseudo\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        displacements = self._calculate_displacement(X)\n",
    "        return np.where(displacements > 0, 1 if self.class_1_pseudo > 0 else 0, 0 if self.class_1_pseudo > 0 else 1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        displacements = self._calculate_displacement(X)\n",
    "        raw_probs = 1 / (1 + np.exp(-displacements * self.class_1_pseudo * 0.5))\n",
    "        return np.column_stack([1 - raw_probs, raw_probs]) if self.class_1_pseudo > 0 else np.column_stack([raw_probs, 1 - raw_probs])\n",
    "\n",
    "\n",
    "class EnsembleSMPA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=5, bootstrap=True, bootstrap_features=False, \n",
    "                 max_features=1.0, max_samples=1.0, random_state=None, verbose=False,\n",
    "                 learning_rate=0.005, epochs=50, lambda_scaling='log', patience=5, \n",
    "                 decay_factor=0.5, min_learning_rate=1e-6, n_control_points=5, \n",
    "                 smoothing_factor=0.1, spline_type='pchip', early_stop_patience=301):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.max_features = max_features\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_scaling = lambda_scaling\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _fit_estimator(self, X, y, sample_indices, feature_indices, seed):\n",
    "        estimator = SMPAEstimator(\n",
    "            learning_rate=self.learning_rate, epochs=self.epochs, random_state=seed,\n",
    "            verbose=self.verbose, lambda_scaling=self.lambda_scaling, patience=self.patience,\n",
    "            decay_factor=self.decay_factor, min_learning_rate=self.min_learning_rate,\n",
    "            n_control_points=self.n_control_points, smoothing_factor=self.smoothing_factor,\n",
    "            spline_type=self.spline_type, early_stop_patience=self.early_stop_patience\n",
    "        )\n",
    "        return {'estimator': estimator.fit(X[sample_indices][:, feature_indices], y[sample_indices]),\n",
    "                'sample_indices': sample_indices, 'feature_indices': feature_indices}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if not set(np.unique(y)).issubset({0, 1}) or X.shape[1] != 2:\n",
    "            raise ValueError(\"Labels must be 0 and 1, and X must be 2D\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        max_samples = int(self.max_samples * n_samples) if isinstance(self.max_samples, float) else min(self.max_samples, n_samples)\n",
    "        max_features = max(1, int(self.max_features * n_features)) if isinstance(self.max_features, float) else min(self.max_features, n_features)\n",
    "\n",
    "        sample_indices_list = [self.rng.choice(n_samples, max_samples, replace=True) if self.bootstrap else np.arange(n_samples) \n",
    "                               for _ in range(self.n_estimators)]\n",
    "        feature_indices_list = [self.rng.choice(n_features, max_features, replace=False) if self.bootstrap_features else np.arange(n_features) \n",
    "                                for _ in range(self.n_estimators)]\n",
    "        seeds = [self.rng.randint(0, 10000) if self.random_state is not None else None for _ in range(self.n_estimators)]\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=-1, verbose=10 if self.verbose else 0)(\n",
    "            delayed(self._fit_estimator)(X, y, sample_indices, feature_indices, seed)\n",
    "            for sample_indices, feature_indices, seed in zip(sample_indices_list, feature_indices_list, seeds)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        predictions = np.array(Parallel(n_jobs=-1)(\n",
    "            delayed(lambda est: est['estimator'].predict(X))(est) for est in self.estimators_\n",
    "        )).T\n",
    "        return np.round(np.mean(predictions, axis=1)).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        probas = np.mean(np.array(Parallel(n_jobs=-1)(\n",
    "            delayed(lambda est: est['estimator'].predict_proba(X))(est) for est in self.estimators_\n",
    "        )), axis=0)\n",
    "        return probas\n",
    "        \n",
    "    def visualize(self, X, y, figsize=(10, 10)):\n",
    "        \"\"\"\n",
    "        Visualize the ensemble decision boundaries.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            The input samples\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values\n",
    "        figsize : tuple, default=(10, 10)\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fig : matplotlib figure\n",
    "            The visualization figure\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Create a figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot data points\n",
    "        ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', label='Class 0', alpha=0.7)\n",
    "        ax.scatter(X[y == 1, 0], X[y == 1, 1], c='coral', label='Class 1', alpha=0.7)\n",
    "        \n",
    "        # Create a mesh grid to visualize decision boundaries\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                             np.arange(y_min, y_max, 0.02))\n",
    "        \n",
    "        # Plot decision boundaries for each estimator\n",
    "        for i, estimator_dict in enumerate(self.estimators_):\n",
    "            estimator = estimator_dict['estimator']\n",
    "            \n",
    "            # Plot the spline curve\n",
    "            x_curve = np.linspace(x_min, x_max, 100)\n",
    "            y_curve = estimator.spline(x_curve)\n",
    "            ax.plot(x_curve, y_curve, 'k-', alpha=0.3, linewidth=1)\n",
    "            \n",
    "        # Plot the ensemble decision boundary\n",
    "        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = self.predict(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot the decision boundary with contour\n",
    "        contour = ax.contour(xx, yy, Z, levels=[0.5], colors='k', linewidths=2)\n",
    "        \n",
    "        # Plot the probability heatmap\n",
    "        proba = self.predict_proba(mesh_points)[:, 1]\n",
    "        proba = proba.reshape(xx.shape)\n",
    "        contourf = ax.contourf(xx, yy, proba, levels=np.linspace(0, 1, 11), \n",
    "                               cmap='RdBu_r', alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(contourf, ax=ax)\n",
    "        cbar.set_label('Probability of Class 1')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_title('Ensemble SMPA Decision Boundary')\n",
    "        ax.legend()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moon_dataset(n_points=400, noise=0.2):\n",
    "    X, y = make_moons(n_samples=n_points, random_state=7, noise=noise)\n",
    "    return X, y\n",
    "\n",
    "class ClassifierComparison:\n",
    "    def __init__(self, X, y, random_state=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.random_state = random_state\n",
    "        # We won't create a single split upfront anymore\n",
    "        # but will keep track of class distribution for reference\n",
    "        self.grids = {\n",
    "            'smpa': {\n",
    "                'learning_rate': [0.001, 0.005],\n",
    "                'epochs': [300],\n",
    "                'n_control_points': [7, 8, 9, 10],\n",
    "                'decay_factor': [0.99, 0.999],\n",
    "                'lambda_scaling': ['log', 'sqrt'],\n",
    "                'patience' : [5, 10],\n",
    "                'n_estimators' : [5, 10, 50],\n",
    "                'spline_type' : ['cubic']\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': [0.1, 1, 10, 50, 100],\n",
    "                'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "                'kernel': ['rbf']\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': [5, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        }\n",
    "        # Define scaler types but don't fit them yet\n",
    "        self.scaler_types = {\n",
    "            'smpa': MinMaxScaler(feature_range=(-100, 100)),\n",
    "            'svm': StandardScaler(),\n",
    "            'rf': StandardScaler(),\n",
    "            'dt': StandardScaler()\n",
    "        }\n",
    "\n",
    "    def create_train_test_split(self, seed):\n",
    "        \"\"\"Create a fresh train-test split with the given random seed\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=seed, stratify=self.y\n",
    "        )\n",
    "        print(f\"Train Class Dist - {np.bincount(y_train)}\")\n",
    "        print(f\"Test Class Dist - {np.bincount(y_test)}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def scale_data(self, X_train, X_test, classifier_type):\n",
    "        \"\"\"Scale data using a fresh scaler for the given classifier type\"\"\"\n",
    "        # Create a new scaler instance of the appropriate type\n",
    "        scaler = self.scaler_types[classifier_type].__class__(**self.scaler_types[classifier_type].get_params())\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "\n",
    "    def grid_search(self, X_train_scaled, y_train, classifier_type='smpa', seed=9):\n",
    "        \"\"\"Perform grid search with the provided scaled training data\"\"\"\n",
    "        classifiers = {\n",
    "            'smpa': EnsembleSMPA(random_state=seed),\n",
    "            'svm': SVC(random_state=seed),\n",
    "            'rf': RandomForestClassifier(random_state=seed),\n",
    "            'dt': DecisionTreeClassifier(random_state=seed),\n",
    "        }\n",
    "        clf = classifiers[classifier_type]\n",
    "        param_grid = self.grids[classifier_type]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "        grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=10, verbose=3)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        return grid_search\n",
    "\n",
    "        # Modify the stability_test method in ClassifierComparison class\n",
    "    def stability_test(self, classifier_type='smpa', n_runs=5, save_plots=True, show_plots=False):\n",
    "        scores = []\n",
    "        best_params_list = []\n",
    "        \n",
    "        for seed in range(n_runs):\n",
    "            print(f\"\\nRun {seed} for {classifier_type}...\")\n",
    "            \n",
    "            # Create a fresh train-test split for each run\n",
    "            X_train, X_test, y_train, y_test = self.create_train_test_split(seed)\n",
    "            \n",
    "            # Scale the data for this specific split\n",
    "            X_train_scaled, X_test_scaled = self.scale_data(X_train, X_test, classifier_type)\n",
    "            \n",
    "            # Grid search on this fresh train split with newly scaled data\n",
    "            grid_search = self.grid_search(X_train_scaled, y_train, classifier_type, seed)\n",
    "            clf = grid_search.best_estimator_\n",
    "            best_params_list.append(grid_search.best_params_)\n",
    "            print(f\"Run {seed} Best Params: {grid_search.best_params_}\")\n",
    "            \n",
    "            try:\n",
    "                y_pred = clf.predict(X_test_scaled)\n",
    "                score = accuracy_score(y_test, y_pred)\n",
    "                scores.append(score)\n",
    "                \n",
    "                # if classifier_type == 'smpa':\n",
    "                #     # Plot with the same scaled data used for training\n",
    "                #     try:\n",
    "                #         fig = clf.visualize(X_train_scaled, y_train)\n",
    "                #         if save_plots:\n",
    "                #             # Save the figure instead of displaying it\n",
    "                #             plt.savefig(f\"{classifier_type}_run_{seed}.png\", dpi=100)\n",
    "                #         if show_plots:\n",
    "                #             plt.show()\n",
    "                #         # Always close the figure to free memory\n",
    "                #         plt.close(fig)\n",
    "                #     except Exception as e:\n",
    "                #         print(f\"🚨 Error in visualization for run {seed}: {e}\")\n",
    "                \n",
    "                print(f\"Run {seed} Score: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"🚨 Error in run {seed}: {e}\")\n",
    "                scores.append(0)\n",
    "        \n",
    "        return {\n",
    "            'mean_score': np.mean(scores),\n",
    "            'std_score': np.std(scores),\n",
    "            'scores': scores,\n",
    "            'best_params_list': best_params_list\n",
    "        }\n",
    "    def statistical_significance_test(self, baseline_scores_dict, target='smpa'):\n",
    "        target_scores = baseline_scores_dict[target]['scores']\n",
    "        for clf_type, results in baseline_scores_dict.items():\n",
    "            if clf_type != target:\n",
    "                t_stat, p_val = stats.ttest_ind(target_scores, results['scores'])\n",
    "                print(f\"\\n🔬 {target} vs. {clf_type}:\")\n",
    "                print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "                print(f\"P-Value: {p_val:.4f}\")\n",
    "                print(\"🏆 Significant difference!\" if p_val < 0.05 else \"🤝 No significant difference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0 for smpa...\n",
      "Train Class Dist - [160 160]\n",
      "Test Class Dist - [40 40]\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10, spline_type=cubic;, score=0.860 total time=  56.6s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10, spline_type=cubic;, score=0.811 total time=  58.0s\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5, spline_type=cubic;, score=0.860 total time=  59.4s\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5, spline_type=cubic;, score=0.821 total time=  59.7s\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=5, spline_type=cubic;, score=0.850 total time= 1.0min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=5, patience=10, spline_type=cubic;, score=0.850 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10, spline_type=cubic;, score=0.821 total time= 2.5min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5, spline_type=cubic;, score=0.811 total time= 2.6min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5, spline_type=cubic;, score=0.850 total time= 2.6min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10, spline_type=cubic;, score=0.860 total time= 2.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=10, spline_type=cubic;, score=0.869 total time= 2.8min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=7, n_estimators=10, patience=5, spline_type=cubic;, score=0.869 total time= 2.9min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5, spline_type=cubic;, score=0.869 total time= 1.0min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5, spline_type=cubic;, score=0.821 total time= 1.0min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=5, spline_type=cubic;, score=0.860 total time= 1.1min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10, spline_type=cubic;, score=0.869 total time= 1.1min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10, spline_type=cubic;, score=0.830 total time= 1.0min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=5, patience=10, spline_type=cubic;, score=0.879 total time= 1.2min\n",
      "[CV 1/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5, spline_type=cubic;, score=0.860 total time= 2.7min\n",
      "[CV 2/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5, spline_type=cubic;, score=0.860 total time= 2.7min\n",
      "[CV 3/3] END decay_factor=0.99, epochs=300, lambda_scaling=log, learning_rate=0.001, n_control_points=8, n_estimators=10, patience=5, spline_type=cubic;, score=0.830 total time= 2.7min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m comparison\u001b[38;5;241m.\u001b[39mgrid_search \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMethodType(grid_search_modified, comparison)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_type \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Set save_plots=True and show_plots=False to avoid VS Code renderer issues\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     results[clf_type] \u001b[38;5;241m=\u001b[39m \u001b[43mcomparison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstability_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_type, res \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Stability:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m, in \u001b[0;36mClassifierComparison.stability_test\u001b[0;34m(self, classifier_type, n_runs, save_plots, show_plots)\u001b[0m\n\u001b[1;32m     90\u001b[0m X_train_scaled, X_test_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_data(X_train, X_test, classifier_type)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Grid search on this fresh train split with newly scaled data\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m clf \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     95\u001b[0m best_params_list\u001b[38;5;241m.\u001b[39mappend(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mgrid_search_modified\u001b[0;34m(self, X_train_scaled, y_train, classifier_type, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Limit the number of parallel jobs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(clf, param_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "X, y = generate_moon_dataset(n_points=400, noise=0.3)\n",
    "comparison = ClassifierComparison(X, y)\n",
    "\n",
    "# Run all classifiers on same split\n",
    "classifiers = ['smpa', 'svm', 'rf', 'dt']\n",
    "results = {}\n",
    "\n",
    "# Reduce the computation load for grid search\n",
    "# Modify the GridSearchCV in the grid_search method\n",
    "def grid_search_modified(self, X_train_scaled, y_train, classifier_type='smpa', seed=0):\n",
    "    \"\"\"Perform grid search with the provided scaled training data\"\"\"\n",
    "    classifiers = {\n",
    "        'smpa': EnsembleSMPA(random_state=seed),\n",
    "        'svm': SVC(random_state=seed),\n",
    "        'rf': RandomForestClassifier(random_state=seed),\n",
    "        'dt': DecisionTreeClassifier(random_state=seed),\n",
    "    }\n",
    "    clf = classifiers[classifier_type]\n",
    "    param_grid = self.grids[classifier_type]\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    # Limit the number of parallel jobs\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    return grid_search\n",
    "\n",
    "# Replace the original method with the modified one\n",
    "comparison.grid_search = types.MethodType(grid_search_modified, comparison)\n",
    "\n",
    "for clf_type in classifiers:\n",
    "    # Set save_plots=True and show_plots=False to avoid VS Code renderer issues\n",
    "    results[clf_type] = comparison.stability_test(clf_type, n_runs=5, save_plots=True, show_plots=False)\n",
    "\n",
    "for clf_type, res in results.items():\n",
    "    print(f\"\\n📊 {clf_type.upper()} Stability:\")\n",
    "    print(f\"Mean Score: {res['mean_score']:.4f}\")\n",
    "    print(f\"Score Std Dev: {res['std_score']:.4f}\")\n",
    "\n",
    "comparison.statistical_significance_test(results, target='smpa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

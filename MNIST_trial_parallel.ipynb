{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import shared_memory\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "\n",
    "class OptimizedBSpline(nn.Module):\n",
    "    \"\"\"Optimized B-spline with iterative De Boor algorithm\"\"\"\n",
    "    def __init__(self, knots, degree=3):\n",
    "        super().__init__()\n",
    "        self.knots = knots\n",
    "        self.degree = degree\n",
    "        self.n_knots = len(knots)\n",
    "\n",
    "    def _find_interval(self, t):\n",
    "        \"\"\"Find the knot interval index for each t value\"\"\"\n",
    "        # Clamp t to valid range and find interval using searchsorted\n",
    "        t_clamped = torch.clamp(t, self.knots[self.degree], self.knots[self.n_knots - self.degree - 1])\n",
    "        i = torch.searchsorted(self.knots, t_clamped, right=True) - 1\n",
    "        # Ensure i stays within valid bounds for degree-k spline\n",
    "        return torch.clamp(i, self.degree, self.n_knots - self.degree - 1)\n",
    "\n",
    "    def forward(self, t, control_points):\n",
    "        \"\"\"Iterative De Boor algorithm for B-spline evaluation\"\"\"\n",
    "        batch_size = t.shape[0]\n",
    "        n_control = len(control_points)\n",
    "        k = self.degree\n",
    "\n",
    "        # Find knot interval for each t\n",
    "        interval = self._find_interval(t)  # Shape: [batch_size]\n",
    "\n",
    "        # Initialize temporary control points for each t\n",
    "        d = torch.zeros(batch_size, k + 1, device=t.device, dtype=t.dtype)\n",
    "        for j in range(k + 1):\n",
    "            idx = torch.clamp(interval - k + j, 0, n_control - 1)\n",
    "            d[:, j] = control_points[idx]\n",
    "\n",
    "        # Iterative De Boor computation\n",
    "        for r in range(1, k + 1):\n",
    "            for j in range(k, r - 1, -1):\n",
    "                left_knot = self.knots[interval - k + j]\n",
    "                right_knot = self.knots[interval - k + j + r]\n",
    "                alpha = (t - left_knot) / (right_knot - left_knot + 1e-8)  # Add epsilon for stability\n",
    "                d[:, j] = (1 - alpha) * d[:, j - 1] + alpha * d[:, j]\n",
    "\n",
    "        return d[:, k]  # Final value for each t\n",
    "\n",
    "class OptimizedTorchCubicSpline(nn.Module):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = nn.Parameter(y)\n",
    "        n = len(x)\n",
    "        self.register_buffer('knots', torch.cat([x[0].repeat(3), x[1:-1], x[-1].repeat(3)]))\n",
    "        self.bspline = OptimizedBSpline(self.knots, degree=3)\n",
    "        self.x_min = x[0]\n",
    "        self.x_max = x[-1]\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.bspline(torch.clamp(t, self.x_min, self.x_max), self.y)\n",
    "\n",
    "class OptimizedDifferentiablePchip(nn.Module):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = nn.Parameter(y)\n",
    "        self.n = len(x) - 1\n",
    "\n",
    "    def _compute_derivatives(self, y):\n",
    "        dy = y[1:] - y[:-1]\n",
    "        dx = self.x[1:] - self.x[:-1]\n",
    "        slopes = dy / dx\n",
    "        d = torch.zeros_like(y)\n",
    "        for i in range(1, len(y)-1):\n",
    "            if slopes[i-1] * slopes[i] > 0:\n",
    "                w1 = 2*dx[i] + dx[i-1]\n",
    "                w2 = dx[i] + 2*dx[i-1]\n",
    "                d[i] = (w1 + w2) / (w1/slopes[i-1] + w2/slopes[i])\n",
    "        d[0] = slopes[0]\n",
    "        d[-1] = slopes[-1]\n",
    "        return d\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.contiguous()\n",
    "        idx = torch.clamp(torch.searchsorted(self.x, t) - 1, 0, self.n - 1)\n",
    "        x0 = self.x[idx]\n",
    "        x1 = self.x[idx + 1]\n",
    "        y0 = self.y[idx]\n",
    "        y1 = self.y[idx + 1]\n",
    "        t_norm = (t - x0) / (x1 - x0)\n",
    "        d = self._compute_derivatives(self.y)\n",
    "        d0 = d[idx]\n",
    "        d1 = d[idx + 1]\n",
    "        t2 = t_norm * t_norm\n",
    "        t3 = t2 * t_norm\n",
    "        h00 = 2*t3 - 3*t2 + 1\n",
    "        h10 = t3 - 2*t2 + t_norm\n",
    "        h01 = -2*t3 + 3*t2\n",
    "        h11 = t3 - t2\n",
    "        dx_segment = x1 - x0\n",
    "        return h00 * y0 + h10 * dx_segment * d0 + h01 * y1 + h11 * dx_segment * d1\n",
    "\n",
    "class OptimizedPyTorchGradientSMPA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.05, epochs=100, random_state=7, verbose=False,\n",
    "                 lambda_reg=0.0001, patience=10, decay_factor=0.9, min_learning_rate=1e-6,\n",
    "                 n_control_points=6, smoothing_factor=0.0001, spline_type='cubic',\n",
    "                 device=None, track_history=False, optimizer_type='adam', scheduler_type='reduce_on_plateau'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.track_history = track_history\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.scheduler_type = scheduler_type\n",
    "\n",
    "        if spline_type not in ['cubic', 'pchip']:\n",
    "            raise ValueError(\"spline_type must be 'cubic' or 'pchip'\")\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def _to_tensor(self, data, dtype=torch.float32):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.to(self.device, dtype=dtype, non_blocking=True)\n",
    "        return torch.tensor(data, dtype=dtype, device=self.device)\n",
    "\n",
    "    def _calculate_class_means(self, X, y):\n",
    "        mask_1 = y == 1\n",
    "        self.m1 = torch.mean(X[mask_1], dim=0)\n",
    "        self.m0 = torch.mean(X[~mask_1], dim=0)\n",
    "\n",
    "    def _initialize_control_points(self, X):\n",
    "        n_features = X.shape[1] - 1\n",
    "        self.spline_models = nn.ModuleList()\n",
    "        for i in range(n_features):\n",
    "            x_min, x_max = X[:, i].min().item(), X[:, i].max().item()\n",
    "            control_x = torch.linspace(x_min, x_max, self.n_control_points, device=self.device)\n",
    "            y_min, y_max = X[:, -1].min().item(), X[:, -1].max().item()\n",
    "            y_mid = (self.m0[-1] + self.m1[-1]) / 2\n",
    "            y_range = y_max - y_min\n",
    "            control_y = torch.empty(self.n_control_points, device=self.device).uniform_(\n",
    "                y_mid - y_range * 0.05, y_mid + y_range * 0.05\n",
    "            )\n",
    "            if self.spline_type == 'cubic':\n",
    "                spline = OptimizedTorchCubicSpline(control_x, control_y).to(self.device)\n",
    "            else:\n",
    "                spline = OptimizedDifferentiablePchip(control_x, control_y).to(self.device)\n",
    "            self.spline_models.append(spline)\n",
    "        self.initial_control_points = [(m.x.clone(), m.y.clone()) for m in self.spline_models]\n",
    "\n",
    "    def _calculate_displacement(self, X):\n",
    "        total_spline = sum(spline(X[:, i]) for i, spline in enumerate(self.spline_models))\n",
    "        return X[:, -1] - total_spline\n",
    "\n",
    "    def _update_pseudo_labels(self, X, y):\n",
    "        m1_displacement = self._calculate_displacement(self.m1.unsqueeze(0))[0]\n",
    "        self.class_1_pseudo = 1 if m1_displacement > 0 else -1\n",
    "        self.class_0_pseudo = -self.class_1_pseudo\n",
    "        return torch.where(y == 1, self.class_1_pseudo, self.class_0_pseudo)\n",
    "\n",
    "    def _create_optimizer_and_scheduler(self):\n",
    "        params = [p for spline in self.spline_models for p in spline.parameters()]\n",
    "        if self.optimizer_type.lower() == 'adam':\n",
    "            optimizer = torch.optim.Adam(params, lr=self.initial_learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(params, lr=self.initial_learning_rate)\n",
    "        if self.scheduler_type.lower() == 'reduce_on_plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=self.decay_factor,\n",
    "                patience=self.patience, min_lr=self.min_learning_rate)\n",
    "        elif self.scheduler_type.lower() == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=self.patience, gamma=self.decay_factor\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            l = np.unique(y)\n",
    "            if len(l) != 2:\n",
    "                raise ValueError(\"Algorithm for binary classification only.\")\n",
    "\n",
    "            if X.shape[1] < 2:\n",
    "                raise ValueError(\"At least 2 features required\")\n",
    "\n",
    "            self.label_mapping = {l[0] : 0, l[1] : 1}\n",
    "\n",
    "            y = np.where(y == l[0], 0, 1)\n",
    "\n",
    "            X_tensor = self._to_tensor(X)\n",
    "            y_tensor = self._to_tensor(y, dtype=torch.long)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self._calculate_class_means(X_tensor, y_tensor)\n",
    "                self._initialize_control_points(X_tensor)\n",
    "\n",
    "            optimizer, scheduler = self._create_optimizer_and_scheduler()\n",
    "\n",
    "            best_error = float('inf')\n",
    "            best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "            best_class_1_pseudo = None\n",
    "\n",
    "            if self.track_history:\n",
    "                self.error_history_ = []\n",
    "                self.control_point_history = [self.initial_control_points]\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                pseudo_labels = self._update_pseudo_labels(X_tensor, y_tensor)\n",
    "                displacements = self._calculate_displacement(X_tensor)\n",
    "\n",
    "                errors = displacements * pseudo_labels <= 0\n",
    "                error_count = errors.sum().item()\n",
    "\n",
    "                if self.verbose and epoch % 5 == 0:\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch}: Errors = {error_count}, LR = {current_lr:.6f}\")\n",
    "\n",
    "                if error_count < best_error:\n",
    "                    best_error = error_count\n",
    "                    best_control_ys = [spline.y.clone() for spline in self.spline_models]\n",
    "                    best_class_1_pseudo = self.class_1_pseudo\n",
    "                    self.best_epoch = epoch\n",
    "                    if error_count == 0 and epoch > 10:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Perfect separation achieved at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "                if self.track_history:\n",
    "                    self.error_history_.append(error_count)\n",
    "                    self.control_point_history.append(\n",
    "                        [(s.x.clone().cpu().numpy(), s.y.clone().detach().cpu().numpy())\n",
    "                        for s in self.spline_models]\n",
    "                    )\n",
    "\n",
    "                if error_count == 0:\n",
    "                    continue\n",
    "\n",
    "                error_indices = torch.where(errors)[0]\n",
    "                X_err = X_tensor[error_indices]\n",
    "                y_err = y_tensor[error_indices]\n",
    "                ti = torch.where(y_err == 1, 1, -1)\n",
    "\n",
    "                spline_values = sum(spline(X_err[:, i]) for i, spline in enumerate(self.spline_models))\n",
    "                loss = torch.mean(torch.relu(1.0 - ti * self.class_1_pseudo * (X_err[:, -1] - spline_values)))\n",
    "\n",
    "                if self.lambda_reg > 0:\n",
    "                    smoothness_penalty = 0\n",
    "                    for spline in self.spline_models:\n",
    "                        y_diff = spline.y[1:] - spline.y[:-1]\n",
    "                        x_diff = spline.x[1:] - spline.x[:-1]\n",
    "                        smoothness_penalty += torch.mean((y_diff / (x_diff + 1e-8))**2)\n",
    "                    loss += self.lambda_reg * smoothness_penalty\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler is None:\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(error_count)\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    if optimizer.param_groups[0]['lr'] <= self.min_learning_rate:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Minimum learning rate reached at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            for spline, best_y in zip(self.spline_models, best_control_ys):\n",
    "                spline.y.data = best_y\n",
    "            self.class_1_pseudo = best_class_1_pseudo\n",
    "        except Exception as e:\n",
    "            print(f\"Error in SMPA fit: {str(e)}\", flush=True)\n",
    "            import traceback\n",
    "            traceback.print_exc(flush=True)\n",
    "            raise  # Re-raise the exception to be caught by parent\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        predictions = torch.where(displacements > 0,\n",
    "                                torch.tensor(1 if self.class_1_pseudo > 0 else 0, device=self.device),\n",
    "                                torch.tensor(0 if self.class_1_pseudo > 0 else 1, device=self.device))\n",
    "\n",
    "        # Convert predictions to numpy\n",
    "        pred_numpy = predictions.cpu().numpy()\n",
    "\n",
    "        # Create reverse mapping from 0 and 1 back to original labels\n",
    "        reverse_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "\n",
    "        # Map the predictions back to original labels\n",
    "        original_predictions = np.array([reverse_mapping[p] for p in pred_numpy])\n",
    "\n",
    "        return original_predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = self._to_tensor(X)\n",
    "        displacements = self._calculate_displacement(X_tensor)\n",
    "        raw_probs = 1 / (1 + torch.exp(-displacements * self.class_1_pseudo * 0.5))\n",
    "        if self.class_1_pseudo > 0:\n",
    "            probs = torch.column_stack([1 - raw_probs, raw_probs])\n",
    "        else:\n",
    "            probs = torch.column_stack([raw_probs, 1 - raw_probs])\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def plot_convergence(self, figsize=(10, 4)):\n",
    "        if not self.track_history or not hasattr(self, 'error_history_'):\n",
    "            print(\"Convergence plotting requires track_history=True and a fitted model.\")\n",
    "            return None\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.plot(self.error_history_, 'b-', label='Errors')\n",
    "        ax.set_title('Error Convergence')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Number of Errors')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OvOSMPAWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.05, epochs=100, random_state=7, verbose=False,\n",
    "                 lambda_reg=0.0001, patience=10, decay_factor=0.9, min_learning_rate=1e-6,\n",
    "                 n_control_points=6, smoothing_factor=0.0001, spline_type='cubic',\n",
    "                 device=None, track_history=False, optimizer_type='adam', scheduler_type='reduce_on_plateau'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.patience = patience\n",
    "        self.decay_factor = decay_factor\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.n_control_points = n_control_points\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.spline_type = spline_type\n",
    "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.track_history = track_history\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.scheduler_type = scheduler_type\n",
    "\n",
    "    def train_pair(self, pair, X, y, counter):\n",
    "        print(f\"Training {counter:02d}/{self.total_combos:02d} classifier: {pair[0]} vs {pair[1]}\")\n",
    "        mask = np.isin(y, [pair[0], pair[1]])\n",
    "        X_pair = X[mask]\n",
    "        y_pair = y[mask]\n",
    "\n",
    "        variances = np.var(X_pair, axis=0)\n",
    "        zero_variance_count = np.sum(variances == 0)\n",
    "        #print(f\"  Pair {pair}: Zero-variance features: {zero_variance_count}\")\n",
    "        #if zero_variance_count > 0:\n",
    "            #print(f\"    Zero-variance indices: {np.where(variances == 0)[0]}\")\n",
    "\n",
    "        valid_features = variances > 0\n",
    "        X_pair_filtered = X_pair[:, valid_features]\n",
    "        print(f\"  X_pair_filtered shape: {X_pair_filtered.shape}\")\n",
    "\n",
    "        unique_labels = np.unique(y_pair)\n",
    "        if len(unique_labels) != 2:\n",
    "            print(f\"Warning: Expected 2 classes but found {len(unique_labels)} for pair {pair}\")\n",
    "            return pair, None\n",
    "\n",
    "        print(f\"  Class distribution: {pair[0]}: {np.sum(y_pair == pair[0])}, {pair[1]}: {np.sum(y_pair == pair[1])}\")\n",
    "\n",
    "        classifier = OptimizedPyTorchGradientSMPA(\n",
    "            learning_rate=self.learning_rate,\n",
    "            epochs=self.epochs,\n",
    "            random_state=self.random_state,\n",
    "            verbose=self.verbose,\n",
    "            lambda_reg=self.lambda_reg,\n",
    "            patience=self.patience,\n",
    "            decay_factor=self.decay_factor,\n",
    "            min_learning_rate=self.min_learning_rate,\n",
    "            n_control_points=self.n_control_points,\n",
    "            smoothing_factor=self.smoothing_factor,\n",
    "            spline_type=self.spline_type,\n",
    "            device=self.device,\n",
    "            track_history=self.track_history,\n",
    "            optimizer_type=self.optimizer_type,\n",
    "            scheduler_type=self.scheduler_type\n",
    "        )\n",
    "        classifier.fit(X_pair_filtered, y_pair)\n",
    "        return pair, {\n",
    "            'model': classifier,\n",
    "            'features': valid_features\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        class_labels = np.unique(y)\n",
    "        class_combos = list(itertools.combinations(class_labels, r=2))  # Convert to list for joblib\n",
    "        self.classifiers = {}\n",
    "        n_classes = len(class_labels)\n",
    "        self.total_combos = n_classes * (n_classes - 1) // 2\n",
    "\n",
    "        # Parallelize pairwise training\n",
    "        results = Parallel(n_jobs=1, backend='loky', verbose=10)(\n",
    "            delayed(self.train_pair)(pair, X, y, i + 1) for i, pair in enumerate(class_combos)\n",
    "        )\n",
    "\n",
    "        # Collect classifiers\n",
    "        for pair, clf_info in results:\n",
    "            if clf_info is not None:\n",
    "                self.classifiers[pair] = clf_info\n",
    "\n",
    "        if not self.classifiers:\n",
    "            raise ValueError(\"No classifiers trained!\")\n",
    "\n",
    "        self.class_labels_ = class_labels\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], len(self.class_labels_)))\n",
    "        for pair, clf_info in self.classifiers.items():\n",
    "            model = clf_info['model']\n",
    "            features = clf_info['features']\n",
    "            X_filtered = X[:, features]\n",
    "            preds = model.predict(X_filtered)\n",
    "            for i, label in enumerate(self.class_labels_):\n",
    "                if label in pair:\n",
    "                    votes[:, i] += (preds == label).astype(int)\n",
    "        return self.class_labels_[np.argmax(votes, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "(0, 1)\n",
      "0 1\n",
      "(0, 2)\n",
      "0 2\n",
      "(1, 2)\n",
      "1 2\n",
      "tested\n"
     ]
    }
   ],
   "source": [
    "y = [0, 1, 1, 0, 2, 0]\n",
    "a = np.unique(y)\n",
    "print(a)\n",
    "combos = itertools.combinations(a, r = 2)\n",
    "for i in combos:\n",
    "  print(i)\n",
    "  print(i[0], i[1])\n",
    "\n",
    "if len(a) != 2:\n",
    "  print(\"tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Extract NumPy arrays\n",
    "# Extract and reshape NumPy arrays\n",
    "X_train = train_dataset.data.numpy().reshape(60000, -1)  # Shape: (60000, 784)\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().reshape(10000, -1)    # Shape: (10000, 784)\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Shuffle training data\n",
    "rng = np.random.RandomState(12)  # For reproducibility\n",
    "shuffle_idx = rng.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]  # Shuffle the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 01/45 classifier: 0 vs 1\n",
      "  X_pair_filtered shape: (12665, 617)\n",
      "  Class distribution: 0: 5923, 1: 6742\n",
      "Epoch 0: Errors = 10188, LR = 0.030000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   0 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m OvO_SMPA \u001b[38;5;241m=\u001b[39m OvOSMPAWrapper(\n\u001b[1;32m      2\u001b[0m   learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[1;32m      3\u001b[0m   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m   track_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m   )\n\u001b[1;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mOvO_SMPA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     15\u001b[0m pred_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[5], line 77\u001b[0m, in \u001b[0;36mOvOSMPAWrapper.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_combos \u001b[38;5;241m=\u001b[39m n_classes \u001b[38;5;241m*\u001b[39m (n_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Parallelize pairwise training\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloky\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pair\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_combos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Collect classifiers\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair, clf_info \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m, in \u001b[0;36mOvOSMPAWrapper.train_pair\u001b[0;34m(self, pair, X, y, counter)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Class distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_pair\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mpair[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_pair\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mpair[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m classifier \u001b[38;5;241m=\u001b[39m OptimizedPyTorchGradientSMPA(\n\u001b[1;32m     47\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     48\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     scheduler_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_type\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pair_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pair, {\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: classifier,\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: valid_features\n\u001b[1;32m     67\u001b[0m }\n",
      "Cell \u001b[0;32mIn[2], line 257\u001b[0m, in \u001b[0;36mOptimizedPyTorchGradientSMPA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    254\u001b[0m y_err \u001b[38;5;241m=\u001b[39m y_tensor[error_indices]\n\u001b[1;32m    255\u001b[0m ti \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(y_err \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m spline_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_err\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspline_models\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ti \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_1_pseudo \u001b[38;5;241m*\u001b[39m (X_err[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m spline_values)))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_reg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 257\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m y_err \u001b[38;5;241m=\u001b[39m y_tensor[error_indices]\n\u001b[1;32m    255\u001b[0m ti \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(y_err \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m spline_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mspline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_err\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, spline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspline_models))\n\u001b[1;32m    258\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ti \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_1_pseudo \u001b[38;5;241m*\u001b[39m (X_err[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m spline_values)))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_reg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m, in \u001b[0;36mOptimizedDifferentiablePchip.forward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     94\u001b[0m t_norm \u001b[38;5;241m=\u001b[39m (t \u001b[38;5;241m-\u001b[39m x0) \u001b[38;5;241m/\u001b[39m (x1 \u001b[38;5;241m-\u001b[39m x0)\n\u001b[0;32m---> 95\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_derivatives(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m)\n\u001b[1;32m     96\u001b[0m d0 \u001b[38;5;241m=\u001b[39m d[idx]\n\u001b[1;32m     97\u001b[0m d1 \u001b[38;5;241m=\u001b[39m d[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/latest/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OvO_SMPA = OvOSMPAWrapper(\n",
    "  learning_rate=0.03,\n",
    "  epochs=200,\n",
    "  random_state=11,\n",
    "  verbose=True,\n",
    "  n_control_points=9,\n",
    "  spline_type='pchip',\n",
    "  track_history=False\n",
    "  )\n",
    "\n",
    "start_time = time.time()\n",
    "OvO_SMPA.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "pred_time_start = time.time()\n",
    "y_pred = OvO_SMPA.predict(X_test)\n",
    "predict_time = time.time() - pred_time_start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_time_start = time.time()\n",
    "y_pred = OvO_SMPA.predict(X_test)\n",
    "predict_time = time.time() - pred_time_start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
